{
  "documents": [
    {
      "title": "Architecture Principles & Conventions",
      "path": "IsoCrates/architecture",
      "content": "# Architecture Principles & Conventions\n\nThis page summarizes the architectural principles and project conventions that shape how IsoCrates is built and operated. It is aimed at developers and operators who need to predict where data lives, how the system starts, and how “derived” features are regenerated across the [[Backend Service Lifecycle]] and [[Jobs, Worker & Regeneration Pipeline]].\n\n## Database-first documents and versioning\n\nIsoCrates treats documentation as database entities rather than a filesystem of Markdown files. In practice, document content, historical versions, and wikilink relationships are persisted as rows, which is why operational guidance in [[Maintenance, Backups & Runbooks]] emphasizes that backups are “mostly database backups” and why schema evolution is handled through [[Database & Migrations]].\n\nThe following diagram illustrates the core persistence convention, where document history and dependencies are first-class relational data. This “rows not files” stance also informs API shape, since creating content and creating versions are explicit operations in the [[Documents & Versions API]] and links are materialized for traversal in [[Wikilinks & Dependency Graph]].\n\n```mermaid\nerDiagram\n  DOCUMENT {\n    string id\n    string path\n    string title\n    datetime deleted_at\n  }\n  VERSION {\n    string version_id\n    string doc_id\n    string content_hash\n    string author_type\n    datetime created_at\n  }\n  DEPENDENCY {\n    string from_doc_id\n    string to_doc_id\n    string link_text\n  }\n\n  DOCUMENT ||--o{ VERSION : \"has many\"\n  DOCUMENT ||--o{ DEPENDENCY : \"links out\"\n  DOCUMENT ||--o{ DEPENDENCY : \"linked from\"\n```\n\n## Fail-fast startup vs degraded runtime\n\nIsoCrates prefers failing fast during startup over running in a partially initialized state. The FastAPI app validates database connectivity and runs migrations during module import, and then performs configuration validation in the application lifespan; if these checks fail, the process exits rather than serving unpredictable behavior.\n\nAt runtime, the system distinguishes between serving requests and reporting health. The `/health` endpoint is designed to be resilient, returning a “degraded” payload when the database probe fails instead of raising a 5xx, which stabilizes load balancer probes while still surfacing an actionable signal for [[Monitoring & Health Checks]].\n\n| Phase | Behavior | Failure mode | Operator action |\n|---|---|---|---|\n| Import time (backend process start) | Probes the database with `SELECT 1` before app creation | Process exits with `SystemExit(1)` and logs masked `DATABASE_URL` plus targeted hints | Fix connectivity and credentials in [[Configuration Overview]] and confirm DB reachability before restarting |\n| Import time (backend process start) | Runs migrations via `run_migrations(engine, Base)` | Process exits with `SystemExit(1)` on `MigrationError` | Inspect migration state and apply the [[Database & Migrations]] runbook before retrying |\n| Lifespan startup | Enforces production configuration via `validate_production_config()` | Process exits early on configuration errors | Align deployment values with [[Deployment & Hardening Checklist]] and restart |\n| Lifespan startup (best effort) | Purges expired trash and optionally old audit entries | Logs warnings but continues serving | Treat cleanup as hygiene, then verify settings and retention expectations in [[Operations Overview]] |\n| Runtime health probe | Returns JSON with `status`, `db`, `document_count`, and uptime | Returns `status=degraded` when DB probe or count fails | Use this as a stable signal for alerting, then follow investigation steps in [[Maintenance, Backups & Runbooks]] |\n\n## Derived artifacts and regeneration\n\nIsoCrates separates authoritative content from computed artifacts such as indexes and graph projections. That convention allows the core document store to remain correct and backup-friendly, while letting derived results be rebuilt on demand through the job system described in [[Jobs, Worker & Regeneration Pipeline]] and surfaced operationally through [[Jobs & Webhooks API]].\n\nThe regeneration worker polls the database queue every ten seconds, claims jobs one at a time, and runs the documentation agent either locally or by executing inside a dedicated container, depending on `AGENT_MODE`. When a job fails, the worker stores a truncated stderr tail in the job record, which is intentionally “small but diagnostic” so operators can triage common agent misconfiguration issues using [[Maintenance, Backups & Runbooks]] without needing direct access to the agent runtime’s full logs.\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `backend/app/main.py` | Fail-fast DB probe, migration-on-start, lifespan checks, `/health` degraded semantics |\n| `backend/worker.py` | Job poll loop, daily refresh enqueueing, agent invocation modes, stderr truncation policy |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "architecture",
      "keywords": [
        "Architecture",
        "Design"
      ],
      "description": "Architecture Principles & Conventions explains IsoCrates’ database-first approach to documents, versions, and wikilink dependencies, and how that choice shapes backups and API design. It also covers the project’s fail-fast startup conventions versus degraded runtime health reporting, and the worker-driven regeneration model for derived artifacts.",
      "author_type": "seed"
    },
    {
      "title": "Getting Started",
      "path": "IsoCrates",
      "content": "# Getting Started\n\nIsoCrates is a documentation knowledge base that stores documents, versions, and wiki-style links in a database, serves them via a [FastAPI](https://fastapi.tiangolo.com/) backend, and presents them through a [Next.js](https://nextjs.org/) frontend. This page covers the smallest local setup that lets you browse the UI, hit the API, and run background regeneration via the [[Jobs, Worker & Regeneration Pipeline]]. For deeper conceptual context, the storage model and link graph are explained in the [[Data Model Overview]] and [[Wikilinks & Dependency Graph]].\n\n## Prerequisites and local run outline\n\nYou will need Python for the backend and worker, Node.js for the frontend, and a reachable database configured via `DATABASE_URL`. The backend is intentionally fail-fast: it validates database connectivity and applies migrations before serving traffic, which makes misconfiguration show up as an immediate crash rather than subtle runtime errors; that behavior is documented in the [[Backend Service Lifecycle]] and grounded in [[Database & Migrations]]. If you want regeneration jobs to run locally, you also need an agent runtime, which is typically started as a Docker container; the orchestration and job semantics are described in the [[Jobs, Worker & Regeneration Pipeline]] and [[Agent Pipeline Overview]].\n\nThe example below uses the repo’s development scripts to bring up a minimal environment. It starts the doc-agent container (so the worker can `docker exec` into it), starts the backend API on port 8000 with a local SQLite database, starts the frontend dev server on port 3000, and then starts the polling worker. If you are only exploring the UI and API without regeneration, you can omit the worker process and still use the system normally.\n\n```bash\n# Terminal 0: start the doc-agent container (this script stays running)\ncp .env.example .env\n$EDITOR .env  # set your LLM API key (for example OPENROUTER_API_KEY)\n./dev-start.sh\n\n# Terminal 1: backend API (creates backend/.env if missing)\n./dev-backend.sh\n\n# Terminal 2: frontend UI\ncd frontend\nnpm install\nnpm run dev\n\n# Terminal 3: worker (polls every 10 seconds and runs jobs sequentially)\ncd backend\nuv run python worker.py\n```\n\n## Verify the system is healthy\n\nOnce the backend is running, verify health through its `/health` probe, which is designed to return a JSON status even when the database is unavailable. This endpoint is the foundation for readiness checks and monitoring, and it is described operationally in [[Monitoring & Health Checks]]; it also reflects the backend’s strict startup validation and migration behavior described in the [[Backend Service Lifecycle]].\n\nRun the command below and interpret the fields directly: `status` should be `healthy`, `db` should be `ok`, and `document_count` should be a non-negative integer (often `0` on a new database).\n\n```bash\ncurl -s http://localhost:8000/health\n# Example healthy response:\n# {\"status\":\"healthy\",\"db\":\"ok\",\"uptime_seconds\":12,\"version\":\"1.0.0\",\"document_count\":0}\n# If you see status=degraded and db=error, the API process is up but it cannot query the database.\n```\n\n## Common first-run failures\n\nFirst-run problems are usually configuration, connectivity, or environment mismatches between the backend, frontend, and worker. The table below maps common symptoms to likely causes and points you to the most relevant page to resolve the root issue.\n\n| Symptom | Likely cause | Next page |\n|---|---|---|\n| Backend exits immediately with a database connection error | `DATABASE_URL` is missing, points at an unreachable PostgreSQL instance, or points at an unwritable SQLite path; the API validates DB connectivity during import and terminates on failure | [[Configuration Overview]] and [[Database & Migrations]] |\n| Backend starts, but `GET /health` returns `{\"status\":\"degraded\",\"db\":\"error\",...}` | The process is running, but runtime queries fail (for example, transient DB outages or permission issues); `/health` intentionally degrades instead of raising 5xx | [[Monitoring & Health Checks]] |\n| Frontend loads, but API requests fail or target the wrong host | `NEXT_PUBLIC_API_URL` is unset and defaults to `http://localhost:8000`, or it does not match where the backend is reachable from the browser | [[Frontend Architecture]] and [[Configuration Overview]] |\n| Worker logs show `docker exec ...` errors such as “No such container: doc-agent” | The worker defaults to `AGENT_MODE=docker` and expects a running container named `doc-agent`; without that runtime, jobs cannot run the agent script | [[Jobs, Worker & Regeneration Pipeline]] and [[Agent Pipeline Overview]] |\n| `./dev-start.sh` exits after creating `.env` and prompts for an API key | The root `.env` is missing required agent credentials (commonly `OPENROUTER_API_KEY`), so the doc-agent container cannot start and the worker has nothing to exec into | [[Configuration Overview]] and [[Agent Pipeline Overview]] |\n| Regeneration jobs fail with authorization errors when writing documents | Authentication is enabled, but the agent/worker is not configured with a write-capable token for the backend API | [[Authentication & Authorization API]] |\n| Backend refuses to start with “STARTUP BLOCKED” in production-like settings | Production config validation rejects insecure defaults (for example, permissive CORS or missing secrets), and the app refuses to serve traffic until fixed | [[Deployment & Hardening Checklist]] and [[Backend Service Lifecycle]] |\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `README.md` | Documented dev ports and basic local run conventions |\n| `dev-start.sh` | Dev agent-container startup and local terminal commands |\n| `dev-backend.sh` | Backend dev runner and default `backend/.env` values |\n| `backend/app/main.py` | Startup DB validation, migrations, and `/health` response fields |\n| `backend/worker.py` | Worker polling loop, agent invocation mode, and job timeout behavior |\n| `frontend/next.config.js` | Default `NEXT_PUBLIC_API_URL` and `basePath` behavior |\n| `docs/DEPLOYING_AT_YOUR_ORGANIZATION.md` | Concrete env var names and deployment expectations for TLS, CORS, and auth |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "guide",
      "keywords": [
        "Guide",
        "How-To"
      ],
      "description": "This Getting Started page shows how to bring up the IsoCrates backend, frontend, and background worker locally, then verify success via the /health endpoint. It is aimed at developers and operators who need a minimal first boot and a quick map from common startup symptoms to the right troubleshooting pages.",
      "author_type": "seed"
    },
    {
      "title": "Jobs & Webhooks API",
      "path": "IsoCrates/api",
      "content": "# Jobs & Webhooks API\n\nThis page documents the API surface that creates and inspects documentation regeneration jobs, plus the inbound webhook endpoint that queues those jobs automatically. It is written for operators and integrators who need to trigger rebuilds, troubleshoot stuck runs, or safely connect GitHub to the IsoCrates [[Jobs, Worker & Regeneration Pipeline]].\n\n## Job control and inspection\n\nIsoCrates models regeneration as a persistent “generation job” record that moves through queued, running, completed, and failed states. Jobs are created either by a webhook call (which may be signature-verified) or by an authenticated manual trigger, and the background [[Jobs, Worker & Regeneration Pipeline]] processes them sequentially. Authentication for manual triggering follows the JWT bearer scheme described in the [[Authentication & Authorization API]], while read-only job inspection accepts authentication optionally.\n\n| Intent | Endpoint | Expected outcome |\n|---|---|---|\n| Trigger regeneration for a repository (manual) | `POST /api/jobs` | Enqueues a new queued job for `repo_url` and returns the created job record. |\n| List recent jobs across the system | `GET /api/jobs?limit=…` | Returns the most recent jobs, newest first, across all repositories. |\n| List recent jobs for a single repository | `GET /api/jobs?repo_url=…&limit=…` | Returns the most recent jobs for the given repository URL, newest first. |\n| Inspect one job in detail | `GET /api/jobs/{job_id}` | Returns the job record including status and any captured error message, or `404` if unknown. |\n\nOperationally, job listing is the primary tool for understanding what the worker is doing and whether failures are transient. When a job fails, the worker stores a truncated stderr snippet with the job record, which is typically enough to diagnose configuration issues such as missing credentials or an unreachable agent runtime.\n\n## Webhook ingestion and safety notes\n\nWebhook ingestion is exposed under `/api/webhooks` and currently supports a GitHub push webhook that queues a job keyed by repository URL and head commit SHA. Signature verification uses HMAC-SHA256 (`X-Hub-Signature-256`) with `GITHUB_WEBHOOK_SECRET`; when `AUTH_ENABLED=true`, the backend refuses to accept unsigned webhooks because they become a privileged write path. Production deployments should treat this endpoint as security-sensitive and apply the controls in the [[Deployment & Hardening Checklist]], while keeping all relevant environment settings centralized in the [[Configuration Overview]].\n\n| Webhook type | Triggers | Required secrets/config |\n|---|---|---|\n| GitHub push webhook | `X-GitHub-Event: push` to `POST /api/webhooks/github` | `GITHUB_WEBHOOK_SECRET` for signature verification; `AUTH_ENABLED=true` requires the secret to be set. |\n\nThe job enqueue operation deduplicates only when a commit SHA is present, which prevents multiple identical GitHub deliveries from creating concurrent runs. Daily refresh jobs intentionally omit a commit SHA so they are not blocked by deduplication, and the agent is expected to no-op quickly when nothing changed.\n\n## How the worker and agent runtime execute jobs\n\nThe worker (`backend/worker.py`) runs an infinite poll loop that checks for the oldest queued job, marks it running, and then executes the documentation agent. By default it runs in a Docker-exec mode that shells into the `doc-agent` container, which isolates the [OpenHands](https://docs.openhands.dev/) runtime from the API process and keeps job execution out of the request path; this end-to-end flow is described at a higher level in the [[Agent Pipeline Overview]].\n\nJob execution is intentionally single-threaded: the worker processes one job at a time and sleeps between polls, which makes job ordering predictable and reduces contention on derived artifacts such as the [[Wikilinks & Dependency Graph]]. A single job has a fixed timeout (30 minutes by default), and failures are automatically re-queued once before being marked permanently failed, which is why operators often see a short burst of retries when a transient network dependency is flaky.\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `backend/app/main.py` | Router inclusion, auth/security startup warnings, and config naming conventions |\n| `backend/app/api/jobs.py` | Jobs endpoints and auth requirements for manual triggers |\n| `backend/app/api/webhooks.py` | GitHub webhook endpoint, signature verification, and queueing semantics |\n| `backend/app/services/job_service.py` | Deduplication, claim/complete/fail transitions, and retry behavior |\n| `backend/worker.py` | Poll loop, agent invocation mode, timeout, and stderr truncation |\n| `backend/app/api/__init__.py` | Confirmation that jobs and webhooks routers are part of the API surface |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "api",
      "keywords": [
        "API",
        "Reference",
        "Endpoints"
      ],
      "description": "This page covers the Jobs & Webhooks API used to queue and inspect documentation regeneration runs in IsoCrates, including the manual job endpoints and the GitHub push webhook endpoint. It highlights the security and configuration requirements for webhook signature verification and summarizes how the worker executes queued jobs via the agent runtime.",
      "author_type": "seed"
    },
    {
      "title": "Documents & Versions API",
      "path": "IsoCrates/api",
      "content": "# Documents & Versions API\n\nThis page documents the core REST endpoints for working with documents and their version history in the IsoCrates FastAPI backend. It is aimed at developers integrating the frontend, the [[Agent Pipeline Overview]], or external tooling such as the [[MCP Server Integration]] with the canonical persistence layer in the database. It focuses on the document lifecycle endpoints under `/api/docs` and the version history endpoints under `/api/docs/{doc_id}/versions`, while linking out to [[Authentication & Authorization API]] and [[Wikilinks & Dependency Graph]] for the surrounding rules and semantics.\n\n## Document lifecycle (create, update, soft-delete, restore, permanent delete)\n\nThe documents API is rooted at `/api/docs` and is implemented as thin HTTP handlers that delegate lifecycle logic to the domain service layer; the `documents.py` module explicitly describes this as a path permission checked surface. When [[Authentication & Authorization API]] is enabled (`AUTH_ENABLED=true`), the system expects a Bearer token for write operations and then scopes reads and writes to the caller’s allowed path prefixes, which is part of the broader [[Architecture Principles & Conventions]] for least-privilege access.\n\nThe table below summarizes the lifecycle operations that most clients need. In practice, create and update typically produce a new version snapshot and may trigger background work such as embedding generation for search and similarity features, while delete operations distinguish between reversible “trash” and irreversible removal.\n\n| Operation | Endpoint (method and path) | Side effects | Related pages |\n|---|---|---|---|\n| Create document | `POST /api/docs` | Persists a document row and returns the canonical `id`; commonly creates the initial version snapshot and kicks off derived data generation such as embeddings | [[Data Model Overview]] explains document fields; [[Document Registry & Stable IDs]] covers stable ID generation |\n| Read document | `GET /api/docs/{doc_id}` | Returns the current document content and metadata, subject to path-based visibility checks | [[Document Viewing, Editing & Versions UX]] describes how the UI consumes the document payload |\n| Update document content | `PUT /api/docs/{doc_id}` | Updates the document and commonly appends a new version snapshot; provenance fields in versions are used to distinguish human edits from automated writes | [[Document Viewing, Editing & Versions UX]] for editor behavior; [[Agent Pipeline Overview]] for automated updates |\n| Soft-delete (move to trash) | `DELETE /api/docs/{doc_id}` | Marks the document as deleted (idempotent) without removing historical data, enabling restore workflows | [[Operations Overview]] describes operator expectations for retention and cleanup |\n| List trash | `GET /api/docs/trash` | Returns only soft-deleted documents the caller is allowed to see, enabling a trash UI and bulk cleanup flows | [[Document Viewing, Editing & Versions UX]] for the trash experience |\n| Restore from trash | `POST /api/docs/{doc_id}/restore` | Clears the deleted marker and makes the document visible again at its prior path | [[Document Viewing, Editing & Versions UX]] for restore behavior |\n| Permanent delete | `DELETE /api/docs/{doc_id}/permanent` | Irreversibly removes the document (and its associated history) from the database | [[Maintenance, Backups & Runbooks]] for recovery expectations via database restore |\n\nSupporting endpoints often used by clients include `GET /api/docs/search/` for query and keyword filtered search, `GET /api/docs/{doc_id}/versions/latest` when a client wants the latest provenance without listing all history, and `POST /api/docs/batch` for permission-aware multi-document operations. For clients that need optimistic concurrency, `PUT /api/docs/{doc_id}` accepts an optional `version` field and can reject conflicting writes when the caller’s version is stale. If your integration needs deterministic identifiers before content exists, `POST /api/docs/generate-id` provides a stable ID derived from repository and path inputs, which aligns with the move and rename resilience described in [[Document Registry & Stable IDs]].\n\n## Versions and provenance (author_type, author_metadata)\n\nIsoCrates models version history as a document-scoped collection of immutable snapshots. The versions API is nested under the document identifier at `/api/docs/{doc_id}/versions`, which makes it straightforward for the UI to present a per-document timeline and for tools to request either the full history or the latest version only.\n\nProvenance is carried primarily through `author_type` and `author_metadata`. Clients use this provenance to label a version as human-authored versus AI-authored, to display the generator context, and to support workflows where automated regeneration avoids unintentionally overwriting intentional human edits.\n\n| Field | Meaning | Used by |\n|---|---|---|\n| `version_id` | Unique identifier for the version snapshot | Version history navigation and deep links in the UI |\n| `doc_id` | Identifier of the document this version belongs to | Integrity checks and API routing under `/api/docs/{doc_id}/versions` |\n| `created_at` | Timestamp when the snapshot was recorded | Timeline ordering and “Current” labeling in the UI |\n| `author_type` | Origin category such as `ai` or `human` | Badges in [[Document Viewing, Editing & Versions UX]] and guardrails in [[Agent Pipeline Overview]] |\n| `author_metadata` | Optional structured metadata about the authoring process, such as model or generator identifiers | UI display and downstream tooling, including the [[MCP Server Integration]] |\n| `content` | The document content captured at that point in time | Historical rendering, review, and audit workflows |\n| `content_hash` | Digest of content used to detect exact matches | Deduplication and provenance verification in automation workflows |\n\n## How this API interacts with links and jobs\n\nDocument content typically contains wiki-style links that are treated as first-class structure, not just markup. The documents API exposes `GET /api/docs/resolve/` to resolve a wikilink-like target string to a concrete document ID, while graph-shaped link traversal and dependency listings are handled by the endpoints documented in [[Dependencies & Graph API]]; conceptually, this is the runtime manifestation of the model described in [[Wikilinks & Dependency Graph]].\n\nWriting document content also drives derived artifacts that are regenerated asynchronously. The background worker described in [[Jobs, Worker & Regeneration Pipeline]] can enqueue and run regeneration tasks that rebuild search, similarity, and dependency-derived views, and administrative endpoints such as `POST /api/docs/reindex` exist to force rebuilds when needed. Operationally, this means a successful write can be immediately durable in the database while secondary indexes converge shortly afterward, which is important when diagnosing discrepancies between raw document content and graph or search results.\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `backend/app/api/documents.py` | Document CRUD, trash, restore, permanent delete, and supporting endpoints |\n| `backend/app/api/versions.py` | Version listing and retrieval endpoints under `/api/docs/{doc_id}/versions` |\n| `backend/app/schemas/document.py` | Document request and response schema, including conflict detection fields |\n| `backend/app/schemas/version.py` | Version response schema and provenance fields |\n| `backend/app/main.py` | Router mounting, auth expectations, and application-level API description |\n| `frontend/app/docs/trash/page.tsx` | Frontend trash workflow consuming restore and permanent delete |\n| `frontend/app/docs/[docId]/versions/page.tsx` | Frontend version list rendering `author_type` and `author_metadata` |\n| `mcp-server/src/isocrates_mcp/server.py` | MCP tool surface mapping to documents and versions endpoints |\n| `mcp-server/src/isocrates_mcp/api_client.py` | Concrete REST paths used by MCP client for docs, search, resolve, and versions |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "api",
      "keywords": [
        "API",
        "Reference",
        "Endpoints"
      ],
      "description": "This page documents the Documents & Versions API, covering the document lifecycle endpoints under /api/docs (create, update, trash, restore, permanent delete) and the version history endpoints under /api/docs/{doc_id}/versions. It also explains how version provenance fields such as author_type and author_metadata are consumed by the UI and how document writes relate to wikilink resolution and regeneration jobs.",
      "author_type": "seed"
    },
    {
      "title": "Deployment & Hardening Checklist",
      "path": "IsoCrates/operations",
      "content": "# Deployment & Hardening Checklist\n\nThis page is a production gate for deploying IsoCrates, a documentation knowledge base served by a FastAPI backend and a Next.js frontend. It is written for operators who need a compact, verifiable checklist that ties deployment decisions to security outcomes and operational stability. For conceptual background on how components fit together, read [[Operations Overview]] and [[Architecture Principles & Conventions]] alongside this checklist.\n\n## Pre-deploy checklist (must-haves)\n\nUse this section as a release gate before exposing the system beyond localhost. IsoCrates is intentionally fail-fast: the backend validates database connectivity and runs migrations during import, and it blocks startup when production configuration is unsafe, so most “almost configured” deployments end up crash-looping rather than partially working. That behavior is an advantage when you validate each prerequisite explicitly.\n\n| Item | Why it matters | How to validate | Related pages |\n|---|---|---|---|\n| Stable `DATABASE_URL` pointing at production storage | All documents, versions, and wikilinks are stored in the database, so a wrong target means data loss or a split-brain environment | Confirm the backend logs show “Database connection verified”, then call `GET /health` and check `db=ok` and a plausible `document_count` | [[Database & Migrations]], [[Backend Service Lifecycle]] |\n| Migrations succeed on startup | Schema drift breaks write paths and can cause startup failure | Restart the backend and verify logs indicate migrations were applied or that there were no pending migrations | [[Database & Migrations]], [[Backend Service Lifecycle]] |\n| Authentication enabled for any network exposure (`AUTH_ENABLED=true`) | With auth disabled, write endpoints are unprotected and any caller can modify content | Call an endpoint that modifies documents and verify it requires a valid bearer token when auth is enabled | [[Authentication & Authorization API]] |\n| Strong JWT signing secret (`JWT_SECRET_KEY` not default) | Predictable secrets allow token forgery, which bypasses authorization checks | In backend logs, ensure there is no warning about using the default development key when auth is enabled; rotate the secret if you suspect exposure | [[Configuration Overview]], [[Authentication & Authorization API]] |\n| Webhook secret set when using GitHub automation (`GITHUB_WEBHOOK_SECRET`) | Unsigned webhooks can be spoofed to trigger unauthorized regeneration workloads | Ensure the secret is set in the environment and the GitHub webhook configuration uses the same value | [[Jobs & Webhooks API]], [[Configuration Overview]] |\n| Explicit CORS allowlist (`CORS_ALLOWED_ORIGINS`) | Over-broad CORS enables hostile websites to make authenticated browser requests | Verify the allowed origins list contains only your real frontend origins and does not include localhost in production | [[Configuration Overview]], [[Backend Service Lifecycle]] |\n| Reverse proxy terminates TLS and forwards client IP (`X-Forwarded-For`) | TLS is required for tokens in transit, and correct client IPs are needed for per-client controls behind a proxy | Confirm HTTPS is enforced at the edge and that requests reaching the backend include `X-Forwarded-For` | [[Configuration Overview]], [[Monitoring & Health Checks]] |\n| Frontend points at the correct backend URL (`NEXT_PUBLIC_API_URL`) | A mispointed frontend silently sends tokens and writes to the wrong backend | Confirm `frontend/next.config.js` resolves the correct value in your build environment, then load the UI and verify it reads and writes against the intended host | [[Frontend Architecture]], [[Configuration Overview]] |\n| Backups are configured and tested | The database is the primary artifact; without verified backups, incidents become irreversible | Run `./scripts/backup.sh` and confirm it produces a non-trivial file; periodically restore into an isolated environment and verify `GET /health` returns `db=ok` | [[Maintenance, Backups & Runbooks]], [[Database & Migrations]] |\n\n## Security posture and common footguns\n\nIsoCrates hardens itself primarily through strict startup validation and consistent request-time authorization. The backend enforces production configuration correctness during the [[Backend Service Lifecycle]] and exits early with a clear error when settings are unsafe, which makes misconfigurations obvious but also makes orchestration restart policies and alerting important. Configuration and secret handling should follow [[Configuration Overview]], which emphasizes explicit CORS allowlists and avoiding development defaults.\n\nThe highest-risk footgun is leaving authentication off when the service is reachable by untrusted clients. The effective rule is that “dev convenience equals production vulnerability”: enabling auth without rotating away from a default signing secret is also dangerous because it can create a false sense of security. Treat the auth model as an API contract and verify it at the HTTP boundary using [[Authentication & Authorization API]], then treat the reverse proxy as part of the security perimeter by enforcing TLS and forwarding `X-Forwarded-For` correctly.\n\n## Operational readiness (backups, monitoring, incident response)\n\nOperational readiness starts with proving you can recover the database, because documents and versions are first-class database rows rather than files on disk. The short path is to schedule the provided backup script and align your restore drill with the runbook in [[Maintenance, Backups & Runbooks]], then treat successful restore validation as part of release readiness rather than a future task. When you enable retention behaviors such as trash purging and audit log pruning, confirm the expectations and failure modes in [[Operations Overview]] so “best-effort” cleanup warnings do not mask real data lifecycle issues.\n\nMonitoring should couple liveness checks with domain sanity checks. The backend’s `GET /health` endpoint is designed to be load-balancer friendly by returning a degraded status when database probes fail rather than raising a 5xx, and it reports a `document_count` that can catch “empty database” miswires early; use that contract as the basis for [[Monitoring & Health Checks]] alerts. For incident response involving write failures or missing updates, assume the database and migration layer first, then use the operational playbooks in [[Maintenance, Backups & Runbooks]] to decide whether to rollback, restore, or run a controlled restart.\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `docs/DEPLOYING_AT_YOUR_ORGANIZATION.md` | Deployment prerequisites, security configuration, proxy guidance |\n| `backend/app/main.py` | Fail-fast startup validation, migrations, `/health` semantics |\n| `frontend/next.config.js` | Frontend runtime configuration for API base URL and base path |\n| `scripts/backup.sh` | Backup procedure for SQLite and PostgreSQL with retention pruning |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "operations",
      "keywords": [],
      "description": "This page provides a concise Deployment & Hardening Checklist for IsoCrates, focusing on pre-deploy must-haves, high-risk security misconfigurations, and operational readiness checks you can validate before production exposure. It is aimed at operators who need a practical gate tied to the backend startup lifecycle, auth/CORS configuration, backups, and health monitoring.",
      "author_type": "seed"
    },
    {
      "title": "MCP Server Integration",
      "path": "IsoCrates",
      "content": "# MCP Server Integration\n\nThis page covers how the `mcp-server/` package exposes IsoCrates documentation through an MCP (Model Context Protocol) server for AI-enabled editors. It is aimed at operators wiring an editor to a running IsoCrates backend, and at developers who want to understand which backend APIs the MCP tools exercise.\n\nThe MCP server runs over stdio using `FastMCP`, then translates tool calls into HTTP requests against the IsoCrates REST API. It returns markdown strings formatted for LLM consumption, which means the MCP layer is intentionally thin and most behavior is defined by the backend APIs described in [[API Overview]] and operational expectations described in [[Operations Overview]].\n\n```mermaid\nsequenceDiagram\n  participant Editor as MCP client (editor)\n  participant MCP as isocrates-mcp (stdio)\n  participant API as IsoCrates backend (HTTP)\n  Editor->>MCP: tools/call (e.g., get_document)\n  MCP->>API: REST call(s) under /api/docs\n  API-->>MCP: JSON\n  MCP-->>Editor: formatted markdown\n```\n\nThis diagram shows the request path from an MCP client to the backend.\n\n## What the MCP server provides (tool catalog)\n\nThe server registers a small set of read and write tools, each implemented as an async wrapper that calls an `IsoCratesClient` method and then formats the JSON response into markdown. Title-first lookup is a common pattern: tools that accept `title_or_id` attempt wikilink-style resolution by calling the backend resolver first, then fall back to treating the input as a document ID.\n\nThe table summarizes the tool surface exposed to MCP clients and how each maps onto the underlying REST API.\n\n| MCP tool | Purpose | Backend endpoint(s) | Notes |\n|---|---|---|---|\n| `search_docs` | Full-text search across documents | `GET /api/docs/search/` | Supports `path_prefix`, `limit`, and keyword filtering (keywords are sent as a comma-separated query parameter). |\n| `get_document` | Retrieve a document’s current content | `GET /api/docs/resolve/`, then `GET /api/docs/{doc_id}` | Tries title resolution first; on failure, attempts direct ID lookup and returns a user-facing “not found” message. |\n| `list_documents` | List documents, optionally under a path prefix | `GET /api/docs` | Used for browsing by folder-like paths, which aligns with the document organization described in [[Document Viewing, Editing & Versions UX]]. |\n| `get_related` | Show incoming and outgoing wikilink relationships | `GET /api/docs/{doc_id}/dependencies`, `POST /api/docs/batch-titles` | Fetches the dependency edges, then resolves all referenced IDs to titles in one batch call to avoid per-document lookups. |\n| `find_similar_docs` | Find semantically similar documents | `GET /api/docs/{doc_id}/similar` | Depends on embeddings being configured in the backend; empty results are surfaced as “no similar documents found.” |\n| `get_document_sources` | Show provenance for the latest version | `GET /api/docs/{doc_id}/versions/latest` | Formats `author_metadata` such as source file paths, hashes, commit SHA, and model identifiers when present. |\n| `create_document` | Upsert a document (create or update by title and path) | `POST /api/docs` | Sends `author_type=\"human\"` with default `author_metadata={\"source\":\"mcp\"}` so autonomous regeneration can distinguish human edits. |\n| `update_document` | Update an existing document by title or ID | `GET /api/docs/resolve/`, then `PUT /api/docs/{doc_id}` | Resolves the title first, then updates content (and optionally description) with `author_type=\"human\"`. |\n\n## Runtime configuration and authentication\n\nRuntime settings are provided through environment variables and are read when the MCP process starts. The MCP server always talks to the backend over HTTP, so correctness depends on using a reachable `ISOCRATES_API_URL` and matching the backend’s auth configuration described in [[Configuration Overview]].\n\nWhen the backend has auth enabled, the MCP client must send a Bearer token in `ISOCRATES_API_TOKEN`, following the token model described in [[Authentication & Authorization API]]. If connectivity is unstable, the HTTP client retries up to three attempts with exponential backoff (approximately one, two, then four seconds); because HTTP status exceptions are retried, authentication failures can appear as a brief delay before the tool returns an error string.\n\n| Environment variable | Default | Effect |\n|---|---|---|\n| `ISOCRATES_API_URL` | `http://localhost:8000` | Sets the backend base URL used for all REST calls under `/api/docs`. |\n| `ISOCRATES_API_TOKEN` | empty | When set, adds `Authorization: Bearer <token>` to every request; required when backend auth is enforced. |\n| `ISOCRATES_API_TIMEOUT` | `30` | Per-request timeout in seconds for backend calls; timeouts are treated as retryable failures. |\n\n## How MCP usage relates to core IsoCrates concepts\n\nMCP reads and writes operate on the same underlying document resources exposed by the [[Documents & Versions API]]. In practice, `get_document` returns the current document content, while `create_document` and `update_document` create a new versioned state in the backend rather than mutating history in place, which is why provenance queries use the latest version endpoint.\n\nGraph-oriented tools are direct views onto the wikilink dependency model described in [[Wikilinks & Dependency Graph]] and served through the [[Dependencies & Graph API]]. `get_related` should be interpreted as “what the backend currently believes the link graph to be,” so stale results usually indicate that derived artifacts have not been regenerated, a workflow that is operationally owned by the [[Jobs, Worker & Regeneration Pipeline]].\n\nFinally, the MCP interface consistently surfaces document IDs in formatted output, which helps disambiguate pages with similar titles and enables stable references across renames. Those stability conventions are captured in [[Document Registry & Stable IDs]] and matter most when an editor workflow mixes title-based lookup with ID-based updates.\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `mcp-server/README.md` | Installation, configuration variables, client setup examples |\n| `mcp-server/pyproject.toml` | Console entry point and runtime dependencies |\n| `mcp-server/src/isocrates_mcp/server.py` | Tool registration, title resolution behavior, stdio transport |\n| `mcp-server/src/isocrates_mcp/api_client.py` | REST endpoint mapping, auth headers, retry and timeout behavior |\n| `mcp-server/src/isocrates_mcp/formatters.py` | Markdown formatting conventions for tool responses |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "integration",
      "keywords": [],
      "description": "This page documents MCP Server Integration in IsoCrates, explaining how the stdio-based MCP server maps editor tool calls to the backend REST API and returns formatted markdown. It summarizes the exposed tool catalog, runtime environment variables for authentication and timeouts, and how MCP operations relate to documents, versions, and the wikilink dependency graph.",
      "author_type": "seed"
    },
    {
      "title": "Wikilinks & Dependency Graph",
      "path": "IsoCrates/architecture",
      "content": "# Wikilinks & Dependency Graph\n\nThis page explains how IsoCrates treats wiki-style links as a dependency graph, and how that graph is resolved, kept hygienic, and rebuilt. It is intended for authors who write pages with `wikilinks` and for operators diagnosing why “related pages” or graph views look incomplete.\n\n## Wikilinks as dependencies (conceptual model)\n\nIn IsoCrates, a `wikilink` is not only navigation text; it is also a structural dependency that connects two documents. When document A contains a link to document B, the system records an outgoing edge from A to B and, equivalently, an incoming edge to B from A, which is what tools like the [[Dependencies & Graph API]] and the [[MCP Server Integration]] expose as “outgoing” and “incoming” relationships.\n\nThe diagram shows the directionality that matters for graph queries and “related pages” views.\n\n```mermaid\ngraph TD\n  A[Document A]\n  B[Document B]\n  C[Document C]\n\n  A -->|contains Document B| B\n  C -->|contains Document B| B\n\n  Aout[Outgoing from A: {B}]\n  Bin[Incoming to B: {A, C}]\n\n  A -.-> Aout\n  B -.-> Bin\n```\n\n## Resolution and hygiene (titles, IDs, renames)\n\nWikilinks are written in human-friendly terms, but runtime operations need stable identifiers. In practice, the ecosystem uses title-based lookup at authoring time, then resolves titles to document IDs for retrieval and graph queries, which keeps “related documents” stable even when paths or content change. The agent pipeline also performs link hygiene by stripping invalid targets rather than leaving broken `...` syntax in generated prose.\n\nThe table summarizes common resolution cases and where they are enforced.\n\n| Case | Expected resolution | Where enforced |\n|------|---------------------|---------------|\n| `Some Title` where the title exists | Preserve the wikilink and record an edge to the target document | Agent link hygiene keeps valid targets via `sanitize_wikilinks` in `agent/planner.py`; storage and queries are surfaced via the [[Dependencies & Graph API]] |\n| `Display Text` | Resolve edges to `Target Title` while rendering `Display Text` | Agent link hygiene preserves the full token when `Target Title` is valid in `agent/planner.py` |\n| `Missing Title` | Replace with plain text (no wikilink, no edge) to avoid dangling references | `sanitize_wikilinks` in `agent/planner.py` replaces invalid targets with the display text |\n| Page rename or retitle during regeneration | Update in place rather than creating a duplicate document | Planner contract supports `replaces_title` for renames in `agent/planner.py`; stable identity is discussed in [[Document Registry & Stable IDs]] |\n| Tool input is a title rather than an ID | Resolve title to doc ID first, then fetch document or dependencies by ID | MCP `get_document` and `get_related` resolve via `resolve_wikilink(...)` before ID-based calls in `mcp-server/src/isocrates_mcp/server.py` |\n\n## Where to rebuild or diagnose graph data\n\nIf dependency edges look stale, the first question is whether derived artifacts have been regenerated for the latest document versions. IsoCrates handles regeneration through queued jobs processed by a worker, so graph rebuilds and indexing tend to follow the same operational path as other “derived data” refreshes described in [[Jobs, Worker & Regeneration Pipeline]].\n\nOperationally, graph rebuild and diagnosis usually starts with the APIs that expose edges and job state, then moves to worker logs and runbooks when a refresh does not complete. The [[Dependencies & Graph API]] is the canonical read surface for outgoing and incoming edges, while [[Maintenance, Backups & Runbooks]] is the operator entry point when jobs are failing, when the worker cannot invoke the agent runtime, or when database restores require re-deriving search and graph artifacts.\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `agent/planner.py` | Wikilink sanitization and rename contract (`replaces_title`) |\n| `backend/worker.py` | Job polling, agent invocation mode, and regeneration scheduling |\n| `mcp-server/src/isocrates_mcp/server.py` | Title-to-ID resolution and dependency retrieval in MCP tools |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "architecture",
      "keywords": [
        "Architecture",
        "Design"
      ],
      "description": "This page covers Wikilinks & Dependency Graph in IsoCrates, explaining how `[[wikilinks]]` are treated as directional dependencies with incoming and outgoing edges. It also summarizes how titles are resolved to IDs, how invalid links are sanitized during generation, and where operators go to rebuild or diagnose graph data.",
      "author_type": "seed"
    },
    {
      "title": "Frontend Architecture",
      "path": "IsoCrates/frontend",
      "content": "# Frontend Architecture\n\nThis page covers the IsoCrates web frontend implemented as a [Next.js](https://nextjs.org/) application under `frontend/`. It explains how routes are structured in the App Router, how rendering is split between server and client components, and where the frontend draws its boundary to the backend APIs. It is aimed at developers who need to extend UI workflows such as login, trash management, and version viewing.\n\n## Route map and rendering strategy\n\nIsoCrates uses the Next.js App Router, so each route is implemented as a React component under `frontend/app/`. Client components are explicitly marked with `'use client'` and manage browser-only state such as form inputs, selection sets, and imperative navigation. Server components are written as `async` functions that fetch data during rendering; the version-related routes also opt out of caching via `export const dynamic = 'force-dynamic'`, which keeps historical views and lists fresh when new versions are created.\n\n| Route | Component type | Key responsibilities |\n|---|---|---|\n| `/login` | Client component (`'use client'`) | Collects credentials, calls auth API helpers, persists auth state via a global store, then navigates to `/docs`. |\n| `/docs/trash` | Client component (`'use client'`) | Fetches soft-deleted documents, manages selection for bulk operations, and triggers restore or permanent delete mutations with toast feedback. |\n| `/docs/[docId]/versions` | Server component (`async`, `dynamic='force-dynamic'`) | Fetches version metadata for a document and renders a navigable version history list. |\n| `/docs/[docId]/versions/[versionId]` | Server component (`async`, `dynamic='force-dynamic'`) | Fetches a single historical version and renders its Markdown content with version metadata. |\n\n## State management and API boundary\n\nThe frontend keeps cross-cutting state in global stores exposed as hooks, which allows route components to update shared UI and authentication state without threading props through layout trees. For example, the login flow persists the authenticated user and token through `useAuthStore`, while the trash workflow updates a global trash counter via `useUIStore.getState().setTrashCount(...)` after restore or delete mutations. These stateful concerns are part of the product behavior described in [[Document Viewing, Editing & Versions UX]], because they directly affect navigation, counters, and document lifecycle affordances.\n\nNetwork access is intentionally funneled through a thin API helper layer imported as `@/lib/api/*`, which provides functions such as `login(...)`, `register(...)`, `getTrash()`, `restoreDocument(...)`, and `getVersion(...)`. This boundary is where request shapes, authorization headers, error handling, and endpoint paths should be centralized so the rest of the UI can remain mostly domain-driven and typed. When you change backend endpoints or auth requirements, update the API helper implementations to match [[API Overview]] and align token handling with [[Authentication & Authorization API]] so pages do not duplicate security logic.\n\n## Key UI subsystems (editor, markdown renderer, graph/tree)\n\nThe UI is built around feature components that are imported by routes, plus small utility modules for consistent styling and user feedback. Even when a route is server-rendered, it can still compose rich interactive subcomponents, but those subcomponents must be client components and should keep their own browser state local. The subsystems below are the main “building blocks” that most document workflows depend on.\n\n| Subsystem | Primary libraries | Where used |\n|---|---|---|\n| Editor | Not verified in the inspected files; typically implemented as client components under `frontend/components/` | Used when creating or updating documents in the main document viewing and editing routes described in [[Document Viewing, Editing & Versions UX]]. |\n| Markdown renderer | `MarkdownRenderer` component | Used to render historical version content in `/docs/[docId]/versions/[versionId]`. |\n| Graph and tree navigation | Not verified in the inspected files; navigation is implied by trash page comments and store updates | Used for browsing, selecting, and routing to document and folder nodes, including the Trash entry point used by `/docs/trash`. |\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `frontend/app/login/page.tsx` | Client-side login and registration flow using stores and API helpers |\n| `frontend/app/docs/trash/page.tsx` | Trash UI, bulk operations, and global trash count updates |\n| `frontend/app/docs/[docId]/versions/page.tsx` | Server-rendered version history list with forced dynamic rendering |\n| `frontend/app/docs/[docId]/versions/[versionId]/page.tsx` | Server-rendered historical version viewer using `MarkdownRenderer` |\n| `frontend/types/index.ts` | Shared TypeScript domain types for documents, versions, users, and trees |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "frontend",
      "keywords": [],
      "description": "This page covers the IsoCrates Next.js frontend architecture, focusing on the App Router route map, the split between server and client components, and the state-store and API-helper boundary to the backend. It is aimed at developers extending login, trash, and version-viewing workflows while keeping auth and API concerns centralized.",
      "author_type": "seed"
    },
    {
      "title": "Agent Pipeline Overview",
      "path": "IsoCrates/agent",
      "content": "# Agent Pipeline Overview\n\nThis page explains how IsoCrates generates and refreshes wiki pages using its agent pipeline. It is written for developers and operators who need a clear mental model of how repository analysis becomes persisted documents in the IsoCrates knowledge base, and how this pipeline connects to the runtime job system described in [[Jobs, Worker & Regeneration Pipeline]].\n\n## Tiered pipeline (Scout, Planner, Writer)\n\nThe agent pipeline is a three-tier flow implemented in `agent/openhands_doc.py`, where repository exploration is separated from planning and writing. This separation keeps early stages broad and information gathering, while later stages become increasingly focused on producing short pages that fit into the wiki’s information architecture, aligning with [[Architecture Principles & Conventions]].\n\n```mermaid\ngraph TD\n  R[Target repository on disk] --> S[Tier 0: Scouts]\n  S --> REP[Scout reports]\n  REP --> P[Tier 1: Planner]\n  P --> BP[JSON blueprint]\n  BP --> W[Tier 2: Writers]\n  W --> U[Backend upsert: POST /api/docs]\n```\n\nIn Tier 0, `ScoutRunner` in `agent/scout.py` estimates repository size and chooses between topic-based scouting and module-based scouting based on a budget ratio computed from a token estimate and the configured context window. Tier 1 then uses `DocumentPlanner` in `agent/planner.py` to turn the scout output into a JSON “blueprint” that names each page, its path, and its intended cross-references, providing the structure that later pages such as [[Scouts & Planning]] and [[Configuration Overview]] can elaborate on.\n\nIn Tier 2, writer agents receive per-document briefs derived from that blueprint and produce Markdown pages that are upserted into the IsoCrates backend using `DocumentAPIClient.create_or_update_document()` in `agent/api_client.py`. Because documents are stored and versioned in the database, the resulting pages show up through the normal [[Documents & Versions API]] and participate in derived features like the [[Wikilinks & Dependency Graph]].\n\n## Key design choices (testability, context-window sizing, style constraints)\n\nThe pipeline’s code is structured to make individual tiers easier to reason about and to run under different environments. In particular, the scout and planner tiers inject OpenHands SDK types rather than importing them at module import time, and the scout tier dynamically adapts exploration scope based on repository size so that small repos do not waste budget and large repos do not overflow context.\n\n| Choice | Benefit | Where implemented |\n|---|---|---|\n| Dependency injection of SDK classes into scouts and planner | Enables isolated testing and avoids hard import-time coupling to the SDK | `agent/scout.py` (`ScoutRunner.__init__`), `agent/planner.py` (`DocumentPlanner.__init__`) |\n| Context-window sizing via a budget ratio (token estimate divided by context window) | Selects an exploration strategy that fits the model budget | `agent/scout.py` (`ScoutRunner.run`) |\n| Optional compression of scout reports when combined output is large | Keeps later tiers usable by limiting prompt size while preserving coverage | `agent/scout.py` (`ScoutRunner.run`) |\n| Condenser sizing derived from model context window | Trades off conversation history retention against summarization cost per tier | `agent/openhands_doc.py` (LLMSummarizingCondenser sizing), `agent/prompts.py` (divisor constants) |\n| Strict prose constraints embedded as static prompt fragments | Produces wiki pages that are short and consistent, supporting navigation and search quality | `agent/prompts.py` (`PROSE_REQUIREMENTS` and related requirements) |\n| Wikilink sanitization against a known title set | Prevents broken wiki navigation by rewriting invalid `...` links to display text | `agent/planner.py` (`sanitize_wikilinks`) |\n| Backend-owned stable document ID generation with a local fallback | Keeps updates deterministic across runs, while still allowing offline operation | `agent/api_client.py` (`generate_doc_id`) |\n\n## Operational dependencies (worker/agent runtime/backends)\n\nIn production, the agent pipeline is typically driven by the background worker described in [[Jobs, Worker & Regeneration Pipeline]], rather than being run manually. The worker polls for queued regeneration jobs, claims one at a time, and runs the agent script as a subprocess; when running in the default Docker mode it executes `python /workspace/openhands_doc.py --repo <url>` inside an idle `doc-agent` container via `docker exec`, which is reflected in the deployment wiring documented in [[Operations Overview]].\n\nThis runtime path couples the agent to both the IsoCrates backend and the job system. The worker writes job outcomes back through the job service, including a truncated stderr snippet for diagnosis, and the agent itself upserts finished pages back to the backend over HTTP. Job creation, webhook-triggered regeneration, and polling behavior are part of the API surface described in [[Jobs & Webhooks API]], while successful document writes rely on the same backend persistence and versioning behavior as normal user edits.\n\nOperationally, this means the agent pipeline depends on a reachable backend API endpoint, valid LLM credentials in the agent runtime environment, and correct configuration of worker-to-agent execution mode. When those conditions are not met, jobs will fail or time out, and the first debugging step is usually to inspect the job’s captured error message and verify backend health via [[Monitoring & Health Checks]].\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `agent/scout.py` | Scout tier orchestration, sizing, and strategy selection |\n| `agent/planner.py` | Planner tier blueprint generation and wikilink sanitization |\n| `agent/prompts.py` | Static prompt fragments and pipeline sizing constants |\n| `agent/api_client.py` | Backend upsert and stable document ID generation |\n| `agent/openhands_doc.py` | Three-tier pipeline wiring and tier-specific configuration |\n| `backend/worker.py` | Job polling and agent invocation, including docker exec mode |\n| `docker-compose.yml` | Agent and worker deployment wiring and runtime assumptions |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "agent",
      "keywords": [],
      "description": "This page covers the IsoCrates agent pipeline architecture, explaining how Tier 0 scouts explore a repository, Tier 1 planning produces a JSON documentation blueprint, and Tier 2 writers upsert pages into the backend. It also summarizes key design choices in the code and the operational dependencies between the worker, agent runtime, and backend services.",
      "author_type": "seed"
    },
    {
      "title": "Document Registry & Stable IDs",
      "path": "IsoCrates/agent",
      "content": "# Document Registry & Stable IDs\n\nThis page explains how IsoCrates keeps document identity stable across renames and moves, so automated regeneration can update the right record over time. The core idea is a stable `doc_id` that is independent of a document’s human-facing title and path, and a lightweight registry that can re-find documents even after a file move. This is primarily relevant to the generation tooling in the agent runtime, but the same stable IDs are also the handle used by the backend API.\n\n## Stable ID model and metadata embedding\n\nIsoCrates treats `doc_id` as the durable identity for a document, while titles and folder paths remain editable. In the agent pipeline described in [[Agent Pipeline Overview]], when a document is written to a Markdown filesystem location, the generator appends a small YAML-like “bottom matter” metadata block so the ID travels with the content even if a user renames the file.\n\nThe bottom-matter parser is intentionally simple and only supports single-line `key: value` pairs. As a result, the metadata block should stay flat and conservative; complex YAML features such as nested objects and multi-line strings are not expected to round-trip safely.\n\n| Metadata key | Purpose | Risks / limitations |\n|---|---|---|\n| `id` | Stable identity used for lookups and updates | If missing or duplicated, move detection degrades to slower scanning and may pick the wrong file |\n| `repo_url` | Binds the document to a repository provenance | Changing URL normalization conventions can change derived IDs and complicate cross-run reconciliation |\n| `repo_name` | Convenience display value derived from `repo_url` | Derived field can become stale if `repo_url` changes |\n| `doc_type` | Classifies the document for generation and indexing | Treated as a legacy compatibility input in some ID paths, so taxonomy changes should be handled carefully |\n| `collection` | Optional grouping prefix for generated content | Empty values are skipped, so downstream logic should not assume the key exists |\n| `generated_at` | Records when the content was generated | Not an identity signal; it will change on each regeneration |\n| `generator` | Records which generator produced the content | Freeform string; avoid exotic formatting because parsing is line-oriented |\n| `version` | Metadata schema/version marker for the generator | Parser does not support structured upgrades; changes should be additive and flat |\n\n## ID generation and fallback behavior\n\nStable IDs are generated by calling the backend’s ID generator endpoint in the [[Documents & Versions API]], which makes the backend the single source of truth for the ID algorithm. When the backend cannot be reached, the agent client attempts a deterministic, hash-based local fallback so generation can continue, but this fallback should be treated as best-effort rather than a long-term compatibility contract.\n\nDocument publication to IsoCrates uses a retrying `POST` to `/api/docs`, and it can optionally fall back to writing the rendered Markdown to disk when all retries fail. That filesystem fallback writes only `doc_data[\"content\"]`, so callers typically embed the stable ID into the content first if they want the file artifact to remain re-importable.\n\n```python\nfrom pathlib import Path\n\nfrom api_client import DocumentAPIClient\nfrom doc_registry import create_document_with_metadata, generate_doc_id\n\nrepo_url = \"https://github.com/example/acme\"\ndoc_id = generate_doc_id(\n    repo_url=repo_url,\n    path=\"IsoCrates/agent\",\n    title=\"Document Registry & Stable IDs\",\n)\n\ncontent = \"# Document Registry & Stable IDs\\n\\n...\"\ncontent = create_document_with_metadata(\n    content=content,\n    doc_id=doc_id,\n    repo_url=repo_url,\n    doc_type=\"agent\",\n    collection=\"IsoCrates/agent\",\n)\n\nclient = DocumentAPIClient(api_url=\"http://backend-api:8000\")\nresult = client.create_or_update_document(\n    {\n        \"repo_url\": repo_url,\n        \"repo_name\": \"acme\",\n        \"doc_type\": \"agent\",\n        \"content\": content,\n    },\n    fallback_path=Path(\"/notes/out/document-registry--stable-ids.md\"),\n)\nprint(result[\"status\"])  # created/updated from the API, or \"fallback\" on disk\n```\n\n## Operational and API touchpoints\n\nThe stable `doc_id` is the primary handle used by the backend for retrieval, updates, and version history, which is why most interactions described in the [[Documents & Versions API]] are keyed by document ID even when a UI is presenting titles and paths. This choice also supports wiki-style links and derived artifacts because relationships can be stored against an immutable identifier rather than a mutable string.\n\nIn the automated generation flow described in the [[Agent Pipeline Overview]], the registry and embedded metadata allow writers to update the intended document even after a human has reorganized content. In practice, the `doc_id` in bottom matter is the most reliable “anchor,” while the JSON registry file is only an optimization that can be rebuilt by scanning the vault.\n\nBackups and restores preserve stable IDs because documents and versions are persisted in the database, not as authoritative files on disk. Operational procedures in [[Maintenance, Backups & Runbooks]] therefore focus on database snapshots and post-restore validation; as long as the database is restored intact, the `doc_id` namespace and document history remain consistent.\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `agent/doc_registry.py` | Stable ID metadata embedding and registry lookup logic |\n| `agent/api_client.py` | Backend ID generation call, write retry loop, filesystem fallback |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "agent",
      "keywords": [],
      "description": "This page covers the \"Document Registry & Stable IDs\" mechanism in IsoCrates, including how stable document IDs are generated, embedded into Markdown bottom-matter metadata, and re-discovered after moves or renames. It also explains the agent client’s backend-first ID strategy and the retry and filesystem fallback behavior used when posting documents to the API.",
      "author_type": "seed"
    },
    {
      "title": "Capabilities & User Stories",
      "path": "IsoCrates",
      "content": "# Capabilities & User Stories\n\nThis page summarizes what IsoCrates enables for end users, operators, and integrators, using concise user stories and end-to-end workflows. It is written for readers who want a product-level understanding first, then want to drill into the underlying API contracts and operational guarantees via pages like [[Documents & Versions API]], [[Jobs, Worker & Regeneration Pipeline]], and [[Maintenance, Backups & Runbooks]].\n\nIsoCrates is implemented as a [FastAPI](https://fastapi.tiangolo.com/) backend and a [Next.js](https://nextjs.org/) frontend. In addition to interactive workflows such as viewing version history and managing trash, the repository includes a polling worker that runs regeneration jobs and an optional MCP server that exposes read and write tools for AI editors, as described in [[MCP Server Integration]].\n\n## User stories (by role)\n\nThe table below expresses “what success looks like” for common roles. It connects UI-facing workflows described in [[Document Viewing, Editing & Versions UX]] to operational concerns such as [[Monitoring & Health Checks]], while also highlighting automation surfaces such as the regeneration worker in [[Jobs, Worker & Regeneration Pipeline]] and the AI-editor tool surface in [[MCP Server Integration]].\n\n| Role | User story | Outcome |\n|---|---|---|\n| Reader (engineer, analyst) | As a reader, I want to find and open a document by its stable identifier or its wikilink title, so I can quickly get to the authoritative page. | Readers can navigate to a known document and trust they are seeing the current content. |\n| Author (human) | As an author, I want to update a document and know the previous content is preserved, so I can iterate without losing history. | Every change produces a new version that can be reviewed in version history. |\n| Reviewer | As a reviewer, I want to see when a version was created and whether it was authored by a human or an AI generator, so I can judge provenance. | Version history labels author type and shows timestamps, with optional generator and model metadata when present. |\n| Content recovery user | As a user, I want to restore mistakenly deleted documents or permanently delete them when appropriate, so I can manage retention safely. | The trash UI supports restore and irreversible permanent deletion, including bulk operations with confirmation. |\n| Operator | As an operator, I want a health endpoint that reports degraded state instead of failing probes, so I can diagnose incidents without flapping. | `GET /health` returns `healthy` or `degraded`, includes database status, uptime, and a document count for sanity checks. |\n| Operator (backups) | As an operator, I want a simple backup command that works for both SQLite and PostgreSQL, so I can meet recovery objectives. | `scripts/backup.sh` creates timestamped backups, can read `DATABASE_URL` from `backend/.env`, and prunes backups older than 30 days. |\n| Automation integrator | As an integrator, I want background regeneration jobs to run without manual babysitting, so derived artifacts stay current. | The worker polls for queued jobs about every 10 seconds, runs jobs sequentially with a timeout, and records a truncated error snippet when failures occur. |\n| AI editor or tool builder | As a tool builder, I want programmatic search, retrieval, and safe updates, so an AI can assist without bypassing governance. | The MCP server exposes tools such as `search_docs`, `get_document`, `get_related`, and write tools that default to `author_type=\"human\"` with `author_metadata.source=\"mcp\"`. |\n\n## Capability matrix\n\nThis matrix ties top-level capabilities to their intended audience and the canonical pages that describe the underlying mechanisms. Capabilities that involve policies and guarantees are described in [[Architecture Principles & Conventions]] and enforced at runtime by the [[Backend Service Lifecycle]].\n\n| Capability | What it does | Who it’s for | Key pages |\n|---|---|---|---|\n| Core document API surface | Serves a single REST API that includes routers for documents, versions, folder and tree navigation, and supporting features. | Frontend developers, integrators | [[API Overview]], [[Documents & Versions API]] |\n| Version history and provenance | Displays versions newest-first, labels the newest entry as current, and surfaces author type and optional generator or model metadata. | Authors, reviewers | [[Documents & Versions API]], [[Document Viewing, Editing & Versions UX]] |\n| Trash and recovery workflow | Supports restore and irreversible permanent deletion, with UI support for bulk operations and destructive-action confirmation. | Authors, operators | [[Document Viewing, Editing & Versions UX]], [[Operations Overview]] |\n| Wikilinks and dependency graph | Exposes dependency and graph routers so wikilinks can be treated as navigable relationships and related-document views. | Knowledge managers, integrators | [[Dependencies & Graph API]], [[Wikilinks & Dependency Graph]] |\n| Jobs, webhooks, and regeneration | Provides job and webhook routers that can drive asynchronous regeneration and indexing workflows. | Integrators, operators | [[Jobs & Webhooks API]], [[Jobs, Worker & Regeneration Pipeline]] |\n| Regeneration worker execution | Polls the job queue about every 10 seconds, processes jobs sequentially, and runs the agent either via `docker exec` into a container or as a local subprocess, with a 30 minute timeout. | Operators, platform teams | [[Jobs, Worker & Regeneration Pipeline]], [[Agent Pipeline Overview]] |\n| MCP server tool access | Runs an MCP server over stdio exposing `search_docs`, `get_document`, `list_documents`, `get_related`, `find_similar_docs`, `get_document_sources`, and write tools for create and update. | Tool builders, AI editors | [[MCP Server Integration]], [[API Overview]] |\n| Stable ID generation support | Generates stable document IDs via an API call that is used by tooling clients and automation. | Integrators, tool builders | [[Document Registry & Stable IDs]], [[Documents & Versions API]] |\n| Health checks | Provides `GET /health` with database probe, uptime, version, and a document count, returning degraded status on DB errors instead of raising. | Operators, SRE | [[Monitoring & Health Checks]], [[Backend Service Lifecycle]] |\n| Startup validation, migrations, and backups | Validates database connectivity and runs migrations during service startup, and supports database-first backups via `scripts/backup.sh` for SQLite or PostgreSQL. | Operators, deployers | [[Database & Migrations]], [[Maintenance, Backups & Runbooks]] |\n| Authentication gating | Includes auth routing and supports an auth-enabled mode where write endpoints require a bearer token. | Operators, security | [[Authentication & Authorization API]], [[Configuration Overview]] |\n\n## End-to-end workflows\n\nThe first diagram shows the “authoring to versions” lifecycle as it appears in the application. In practice, the frontend calls backend document endpoints, the backend persists changes, and the UI presents a version history where the newest entry is labeled as current.\n\n```mermaid\nsequenceDiagram\n  participant U as User\n  participant FE as Frontend (Next.js)\n  participant BE as Backend API (FastAPI)\n  participant DB as Database\n\n  U->>FE: Edit or update a document\n  FE->>BE: Create or update document content\n  BE->>DB: Persist update and append a version entry\n  DB-->>BE: Commit success\n  BE-->>FE: Updated document response\n  U->>FE: Open Version History\n  FE->>BE: Fetch versions for docId\n  BE->>DB: Query versions ordered by created_at\n  DB-->>BE: Versions list (newest first)\n  BE-->>FE: Versions list\n  FE-->>U: Render list, label newest as Current\n```\n\nThe second diagram captures the “trash to recovery” workflow. The trash UI loads soft-deleted documents, supports restoring them back into the active set, and requires explicit confirmation before permanent deletion.\n\n```mermaid\nsequenceDiagram\n  participant U as User\n  participant FE as Frontend (Trash UI)\n  participant BE as Backend API (Documents)\n  participant DB as Database\n\n  U->>FE: Open Trash\n  FE->>BE: Fetch trashed documents\n  BE->>DB: Query documents with deleted_at set\n  DB-->>BE: Trash list\n  BE-->>FE: Trash list\n\n  alt Restore\n    U->>FE: Click Restore\n    FE->>BE: Restore document\n    BE->>DB: Clear deleted_at and save\n    DB-->>BE: Commit success\n    BE-->>FE: Success\n    FE-->>U: Remove item from trash list\n  else Permanent delete\n    U->>FE: Click Delete permanently\n    FE-->>U: Confirm irreversible deletion\n    U->>FE: Confirm\n    FE->>BE: Permanent delete document\n    BE->>DB: Permanently delete document\n    DB-->>BE: Commit success\n    BE-->>FE: Success\n    FE-->>U: Remove item from trash list\n  end\n```\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `backend/app/main.py` | Backend startup checks, router registration, and `/health` response shape |\n| `backend/worker.py` | Job polling loop, agent invocation modes, timeout and error capture behavior |\n| `scripts/backup.sh` | Backup command behavior for SQLite and PostgreSQL plus retention pruning |\n| `mcp-server/src/isocrates_mcp/server.py` | MCP tool surface for search, retrieval, related docs, and writes |\n| `mcp-server/src/isocrates_mcp/api_client.py` | MCP client request mappings and default `author_type` and metadata |\n| `frontend/app/docs/trash/page.tsx` | Trash UI behavior for restore and permanent delete, including bulk operations |\n| `frontend/app/docs/[docId]/versions/page.tsx` | Version history UI semantics, including “Current” labeling and author metadata display |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "capabilities",
      "keywords": [
        "Capabilities",
        "User Stories",
        "Features",
        "Use Cases"
      ],
      "description": "This page covers “Capabilities & User Stories” for IsoCrates, describing how readers, authors, reviewers, and operators use documents, versions, and trash workflows. It also summarizes automation and integration surfaces including the regeneration worker, backups, and the MCP server’s search and update tools, and includes two end-to-end workflow diagrams.",
      "author_type": "seed"
    },
    {
      "title": "Configuration Overview",
      "path": "IsoCrates",
      "content": "# Configuration Overview\n\nThis page explains how IsoCrates is configured across the [FastAPI](https://fastapi.tiangolo.com/) backend, the [Next.js](https://nextjs.org/) frontend, and the background worker that runs regeneration jobs. It is written for operators and developers who need to understand where settings come from, which ones are boot blockers, and how misconfiguration shows up at runtime.\n\n## Configuration model and precedence\n\nIsoCrates is primarily configured through environment variables, with the backend using Pydantic settings (a typed configuration layer) to read a `.env` file and then apply process environment variables as overrides. In `backend/app/core/config.py` the settings loader is configured with `env_file = \".env\"` and case-insensitive variable names, which means a working-directory `.env` is a convenient default in development while container orchestration can inject the final values at runtime.\n\nPrecedence is component-specific rather than global. For example, `./scripts/backup.sh` prefers an explicitly provided `DATABASE_URL` in the shell environment and only falls back to parsing `backend/.env` when `DATABASE_URL` is unset, while the frontend’s `NEXT_PUBLIC_API_URL` is read through Next.js configuration and defaults to `http://localhost:8000` when not provided.\n\n| Setting source | Examples | Affects |\n|---|---|---|\n| Backend `.env` file (Pydantic settings) | `.env` with `DATABASE_URL=sqlite:///./isocrates.db` | Default backend settings when running without injected environment |\n| Process environment variables (override) | `DATABASE_URL=postgresql://…`, `AUTH_ENABLED=true` | Overrides backend `.env` and controls backend security posture |\n| Worker environment variables | `AGENT_MODE=docker`, `AGENT_SCRIPT_PATH=/workspace/openhands_doc.py` | How regeneration jobs execute in the [[Jobs, Worker & Regeneration Pipeline]] |\n| Frontend public runtime config | `NEXT_PUBLIC_API_URL=https://docs.yourcompany.com` | API base URL used by the UI in the [[Frontend Architecture]] |\n| File-based secrets for agent credentials | `OPENROUTER_API_KEY_FILE=/run/secrets/openrouter_api_key` | Supplying LLM credentials without plaintext env vars, as described in [[Deployment & Hardening Checklist]] |\n| Script arguments for operational tools | `./scripts/backup.sh /var/backups/isocrates` | Backup output location and retention management in [[Maintenance, Backups & Runbooks]] |\n\n## Critical settings (must be correct for the system to boot)\n\nThe backend is intentionally fail-fast for configuration that would make the service unsafe or unusable. At import time it validates database reachability and applies migrations, and during FastAPI startup it calls production configuration validation; if these checks fail, the process exits rather than serving partial functionality. When `ENVIRONMENT=production`, insecure defaults such as `AUTH_ENABLED=false`, a default `JWT_SECRET_KEY`, or `CORS_ALLOWED_ORIGINS` that still includes localhost values can be treated as boot blockers.\n\nThe table focuses on the minimum settings that commonly block a clean start or immediately break core workflows. The worker process can start even when the agent runtime is unreachable, but regeneration jobs will fail or stall until `AGENT_MODE` and its credentials are correct, which is why those settings are operationally critical.\n\n| Setting | Where it is used | What breaks when wrong |\n|---|---|---|\n| `DATABASE_URL` | Backend database engine connection validation in `backend/app/main.py`, backups in `scripts/backup.sh` | Backend exits at startup on connection failure; backups fail or back up the wrong database |\n| `AUTH_ENABLED` | Backend auth gating and security warnings in `backend/app/main.py` | With auth enabled, clients without a Bearer token see empty or 404-like results; with auth disabled in production, write endpoints are exposed |\n| `audit_retention_days` | Backend lifespan housekeeping in `backend/app/main.py` | If set to a positive value, audit purging runs at startup and failures are logged (non-fatal); if mis-set to a very low value, retention may be unexpectedly aggressive |\n| `AGENT_MODE` and `AGENT_SCRIPT_PATH` | Worker agent invocation in `backend/worker.py` | Jobs fail to run if the worker cannot execute the agent in the expected mode or cannot find the script path |\n| `DAILY_REFRESH_INTERVAL` | Worker periodic enqueue logic in `backend/worker.py` | Refresh jobs run too often or not often enough, affecting how quickly derived artifacts catch up |\n\n## Where configuration shows up in behavior\n\nBackend configuration is most visible during startup and health checks. Database misconfiguration causes immediate process termination during the [[Backend Service Lifecycle]], while security-related settings change warnings versus enforced requirements depending on environment; in either case, `/health` is designed to report a degraded status rather than crashing the handler when the database probe fails.\n\nWorker and agent configuration primarily affects throughput and correctness of derived artifacts such as dependency graphs and search indexes. If `AGENT_MODE` and its runtime dependencies are not aligned with your deployment approach, queued jobs will accumulate and regeneration will fail in ways that are easiest to understand through the operational flow described in [[Jobs, Worker & Regeneration Pipeline]]. When you are configuring production, treat secrets, CORS, and auth as first-class boot requirements even if they are not all “hard” boot blockers, and apply the concrete hardening steps in [[Deployment & Hardening Checklist]] so that defaults do not leak into public-facing environments.\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `backend/app/main.py` | Backend startup validation, migrations, lifespan cleanup, and auth/security behavior |\n| `backend/app/core/config.py` | Settings schema, `.env` loading, and production configuration validation |\n| `backend/worker.py` | Worker environment settings and agent invocation modes |\n| `scripts/backup.sh` | Backup script configuration precedence for `DATABASE_URL` and retention behavior |\n| `frontend/next.config.js` | Frontend configuration surface for `NEXT_PUBLIC_API_URL` and base path |\n| `docs/DEPLOYING_AT_YOUR_ORGANIZATION.md` | Production configuration guidance for auth, secrets, CORS, and API URL wiring |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "config",
      "keywords": [
        "Configuration",
        "Deployment"
      ],
      "description": "This page provides a Configuration Overview for IsoCrates, explaining the configuration sources and precedence across backend, frontend, and worker components. It highlights the small set of critical settings that commonly block startup or break core workflows, and points to deeper operational and lifecycle pages for behavior-level details.",
      "author_type": "seed"
    },
    {
      "title": "Jobs, Worker & Regeneration Pipeline",
      "path": "IsoCrates/architecture",
      "content": "# Jobs, Worker & Regeneration Pipeline\n\nThis page explains how IsoCrates runs asynchronous regeneration work using database-backed jobs and a polling worker process. It is written for operators and developers who need to understand how a regeneration request becomes an agent run, how outcomes are persisted, and where to look when the system is not regenerating as expected.\n\n## Job lifecycle (queued → running → finished/failed)\n\nIsoCrates persists regeneration work in the `generation_jobs` table, which stores a `repo_url`, an optional `commit_sha`, and lifecycle fields such as `status`, timestamps, and an `error_message`. A separate `backend/worker.py` process polls the queue every ~10 seconds, claims the oldest queued job, and executes the documentation agent as a subprocess, so jobs run sequentially by design rather than concurrently.\n\nWhen a job is claimed, it transitions from `queued` to `running` and records `started_at`. On success, the worker marks the job `completed` and records `completed_at`; on failure, the worker records diagnostic output and the job is either re-queued once or marked permanently `failed` depending on its `retry_count`. The worker captures the last ~2000 characters of stderr (enough to include most Python tracebacks) into `error_message`, which is the primary on-record clue for diagnosing agent failures.\n\nThe diagram below shows the steady-state loop and where state is written.\n\n```mermaid\nsequenceDiagram\n  autonumber\n  participant W as Worker (backend/worker.py)\n  participant DB as Database (generation_jobs)\n  participant A as Agent runtime (subprocess / docker exec)\n\n  loop Every ~10s (POLL_INTERVAL)\n    W->>DB: claim_next() finds oldest status=\"queued\"\n    alt job found\n      DB-->>W: job (status set to \"running\", started_at set)\n      W->>A: Run agent for repo_url (sequential execution)\n      alt exit code 0\n        W->>DB: complete(job_id) → status=\"completed\", completed_at set\n      else non-zero exit or exception\n        W->>DB: fail(job_id, error_message)\n        Note over DB: stderr truncated to last ~2000 chars\n        Note over DB: retry_count increments; may re-queue once\n      end\n    else no job\n      W-->>W: sleep(POLL_INTERVAL)\n    end\n  end\n```\n\n## Daily refresh jobs and deduping behavior\n\nIn addition to webhook-triggered work described in the [[Jobs & Webhooks API]], the worker periodically enqueues “daily refresh” jobs so tracked repositories regenerate even when no webhook arrives. On startup and then every `DAILY_REFRESH_INTERVAL` seconds (defaulting to 24 hours), the worker fetches tracked repo URLs and enqueues one job per repo with `commit_sha=None`.\n\nDeduplication is intentionally tied to commit SHA: when `commit_sha` is present, enqueueing skips creating a duplicate job if a `queued` or `running` job already exists for the same `(repo_url, commit_sha)`. Daily refreshes avoid this gate by omitting the commit SHA, which guarantees the refresh job is created; this shifts “do we actually need to regenerate?” into the agent, which is expected to detect “unchanged” cheaply and exit early.\n\n| Refresh job field / behavior | Value in daily refresh | Why it avoids dedupe | Operational consequence |\n|---|---|---|---|\n| `commit_sha` | `NULL` | The enqueue-time dedupe check only runs when a commit SHA is provided. | Refresh jobs are always created, even if the last webhook job already ran. |\n| Job multiplicity | One job per tracked `repo_url` per interval | No stable `(repo_url, commit_sha)` key exists to collapse repeats. | A stuck agent or misconfigured runtime can create a growing queued backlog. |\n| “Nothing changed” handling | Handled by agent logic, not the queue | The worker does not compare repository state; it simply enqueues. | If the agent cannot read repository state, “daily refresh” stops being cheap and becomes noise. |\n\n## Troubleshooting entry points\n\nWhen regeneration appears stalled, start by checking overall service health and database connectivity using the signals described in [[Monitoring & Health Checks]]. The backend’s `/health` endpoint is designed to return `degraded` rather than raising when the database probe fails, which makes it a stable first indicator that job state may not be readable or writable.\n\nNext, confirm that a worker process is running and that its configuration matches the intended runtime. The worker’s poll cadence, daily refresh interval, agent invocation mode, script path, container name, and job timeout are controlled by environment variables, so verifying the deployed values in [[Configuration Overview]] is often faster than reasoning from symptoms; in particular, `AGENT_MODE=docker` requires Docker access and a reachable agent container, while `AGENT_MODE=local` requires the worker host to have all agent dependencies installed.\n\nFinally, inspect job records via the job endpoints documented in the [[Jobs & Webhooks API]] and focus on `status`, timestamps, and `error_message`. A repeating pattern where a job flips from `running` back to `queued` indicates the built-in single retry is being exercised, while a terminal `failed` status indicates retries are exhausted and the recorded stderr snippet should be treated as the canonical failure summary; escalation and recovery steps (including backup-first workflows) should follow the operational guidance in [[Maintenance, Backups & Runbooks]].\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `backend/worker.py` | Poll loop, agent invocation, daily refresh enqueueing, stderr truncation and timeout behavior |\n| `backend/app/services/job_service.py` | Job enqueue, claim, complete, fail, and retry semantics including commit SHA dedupe |\n| `backend/app/models/generation_job.py` | Persisted job fields and allowed status values |\n| `backend/app/main.py` | Health endpoint behavior and database preflight semantics relevant to job troubleshooting |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "architecture",
      "keywords": [
        "Architecture",
        "Design"
      ],
      "description": "This page covers Jobs, Worker & Regeneration Pipeline in IsoCrates, including the queued→running→completed/failed lifecycle persisted in the generation_jobs table and the worker’s ~10s polling, sequential execution, and stderr truncation behavior. It also explains daily refresh jobs, why they omit commit SHAs to avoid deduping, and where operators should start when regeneration stalls.",
      "author_type": "seed"
    },
    {
      "title": "API Overview",
      "path": "IsoCrates/api",
      "content": "# API Overview\n\nIsoCrates exposes a REST API from a [FastAPI](https://fastapi.tiangolo.com/) backend that stores documents, versions, and wiki-style links in a relational database. This page gives a compact map of the API surface and the conventions that cut across endpoints, with pointers to deeper reference pages for full request and response details. It is written for developers integrating the [[Frontend Architecture]], automation in the [[Agent Pipeline Overview]], or external tooling via the [[MCP Server Integration]].\n\n## Resource groups and responsibilities\n\nThe backend mounts multiple routers into a single service process, so most clients interact with one base URL and a set of path-prefixed resource groups. In practice, document CRUD and versioning form the core, while dependencies, folders, personal views, and background jobs support navigation and regeneration workflows described in [[Jobs, Worker & Regeneration Pipeline]].\n\n| Resource group | Typical operations | Primary reference pages |\n|---|---|---|\n| Documents | Create and update Markdown content, list by path prefix, resolve titles, search, generate stable IDs, soft-delete and restore | [[Documents & Versions API]], [[Document Registry & Stable IDs]] |\n| Versions | List and fetch immutable snapshots, inspect latest version metadata for provenance | [[Documents & Versions API]], [[Document Viewing, Editing & Versions UX]] |\n| Wikilinks dependencies and graph | Compute incoming and outgoing wikilink edges, render dependency graph views | [[Dependencies & Graph API]], [[Wikilinks & Dependency Graph]] |\n| Folders and tree | Maintain folder hierarchy and produce a navigable tree representation for UI | [[Documents & Versions API]], [[Frontend Architecture]] |\n| Personal views | Store and retrieve user-curated structures such as personal trees | [[Documents & Versions API]] |\n| Jobs | Enqueue and inspect background regeneration work and its status | [[Jobs & Webhooks API]], [[Jobs, Worker & Regeneration Pipeline]] |\n| Webhooks | Ingest external triggers that enqueue work, typically from repository integrations | [[Jobs & Webhooks API]] |\n| Auth | Register and authenticate users and issue tokens when auth is enabled | [[Authentication & Authorization API]] |\n| Service and health | Read service metadata and perform a DB-aware health probe | [[Monitoring & Health Checks]], [[Backend Service Lifecycle]] |\n\n## Cross-cutting behaviors (auth, errors, pagination/search conventions)\n\nThe API is designed to fail fast on misconfiguration at startup while keeping runtime health probes stable. On import, the service validates database connectivity with a simple probe and runs migrations before accepting traffic, which is part of the strict boot behavior documented in [[Backend Service Lifecycle]] and [[Database & Migrations]]. At runtime, the `GET /health` endpoint is explicitly resilient and returns a degraded status rather than raising an exception when the database cannot be queried.\n\nMany endpoints share conventions for authentication, filtering, and error handling that are easier to learn once and apply everywhere. Authentication uses an HTTP `Authorization: Bearer <token>` header when enabled, while CORS and other environment-dependent behavior are controlled through the settings described in [[Configuration Overview]] and hardened in [[Deployment & Hardening Checklist]]. Document listing and search endpoints commonly accept a `limit` and optionally scope results with `path_prefix`, and search adds a query parameter `q` and an optional comma-separated `keywords` filter.\n\n| Concern | Where documented | Affected clients |\n|---|---|---|\n| Bearer-token authentication, including when reads are allowed without a token | [[Authentication & Authorization API]] | [[Frontend Architecture]], [[MCP Server Integration]], [[Agent Pipeline Overview]] |\n| Startup DB preflight and automatic migrations before serving traffic | [[Backend Service Lifecycle]], [[Database & Migrations]] | [[Operations Overview]], [[Maintenance, Backups & Runbooks]] |\n| Health semantics that prefer “degraded” responses over 5xx for probes | [[Monitoring & Health Checks]] | [[Deployment & Hardening Checklist]], [[Operations Overview]] |\n| Domain exception mapping via a dedicated API error handler | [[Architecture Principles & Conventions]] | [[Frontend Architecture]], [[MCP Server Integration]] |\n| Search and listing filters such as `limit`, `path_prefix`, `q`, and `keywords` | [[Documents & Versions API]] | [[Frontend Architecture]], [[MCP Server Integration]] |\n| Stable document identity via server-side ID generation | [[Document Registry & Stable IDs]] | [[Agent Pipeline Overview]], [[MCP Server Integration]] |\n\n## Client entry points (frontend, agent, MCP)\n\nThe [[Frontend Architecture]] uses the REST API as its single source of truth for browsing, editing, trash workflows, and version viewing, so UI behavior usually corresponds directly to document, folder, and version resources described in [[Document Viewing, Editing & Versions UX]]. Meanwhile, the [[Agent Pipeline Overview]] typically interacts with the same endpoints to upsert generated pages and to preserve continuity across renames by relying on stable IDs, which is why the conventions in [[Document Registry & Stable IDs]] matter even when content is fully automated.\n\nThe [[MCP Server Integration]] is an example of an external client that wraps the REST API into higher-level tools. Its client maps common workflows to concrete endpoints, including `GET /api/docs/search/` for full-text search, `GET /api/docs/resolve/` for wikilink title resolution, `GET /api/docs/{doc_id}/dependencies` for dependency retrieval, `POST /api/docs/batch-titles` to avoid N+1 title lookups, and `POST /api/docs/generate-id` for stable identity generation. Because it runs outside the web UI, it also demonstrates practical concerns such as retries with exponential backoff for transient failures and explicit timeout configuration.\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `backend/app/main.py` | Application startup behavior, middleware, `/health` endpoint, router mounting |\n| `backend/app/api/__init__.py` | Authoritative list of mounted API routers and resource groups |\n| `mcp-server/src/isocrates_mcp/api_client.py` | Concrete REST endpoint paths and client-side conventions (search, resolve, dependencies, retries) |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "api",
      "keywords": [
        "API",
        "Reference",
        "Endpoints"
      ],
      "description": "This page provides an API Overview of the IsoCrates REST backend, summarizing the main resource groups and the conventions that apply across endpoints. It links the API surface to its primary clients, including the frontend, the agent pipeline, and the MCP server.",
      "author_type": "seed"
    },
    {
      "title": "Dependencies & Graph API",
      "path": "IsoCrates/api",
      "content": "# Dependencies & Graph API\n\nThis page describes how IsoCrates represents wiki style links as a dependency graph, and how clients retrieve and rebuild that graph through the backend API. It is intended for developers integrating the graph into the UI or tools, and for operators diagnosing missing or stale links. The underlying concept is the [[Wikilinks & Dependency Graph]] model, where a stored edge represents that one document references another.\n\n## Graph retrieval and dependency views\n\nIsoCrates stores dependencies as rows with a source document ID, a target document ID, and metadata such as `link_type` and optional `link_text`. Most edges are `link_type=\"wikilink\"` and come from parsing `Target` or `display` syntax in document content, which keeps the graph aligned with the editing experience described in [[Documents & Versions API]].\n\n| Capability | Endpoint | Typical consumer |\n|---|---|---|\n| Retrieve incoming and outgoing links for one document (two directional adjacency) | `GET /api/docs/{doc_id}/dependencies` returning `{ outgoing: DependencyResponse[], incoming: DependencyResponse[] }` | Document detail views in the UI and the related pages tool in [[MCP Server Integration]] |\n| Add a dependency edge explicitly (idempotent create with validation) | `POST /api/docs/{doc_id}/dependencies` accepting `DependencyCreate` and returning `DependencyResponse` | Import tooling and advanced integrations that model non wikilink relationships |\n| Fetch the global edge list for graph visualization, filtered by access grants | `GET /api/dependencies` returning `DependencyResponse[]` | Graph overviews in the UI described in [[Frontend Architecture]] |\n| Rebuild all wikilink dependencies from stored document content (admin only) | `POST /api/dependencies/reindex` returning processing counts | Administrative maintenance and recovery workflows |\n| Resolve a wikilink target string to a concrete document ID, respecting read permissions | `GET /api/docs/resolve/?target=...` returning `{ target, doc_id }` | Wikilink resolution in clients that accept titles or repo names |\n| Convert document IDs to titles in one request, omitting unreadable documents | `POST /api/docs/batch-titles` returning `{ \"doc_id\": \"title\", ... }` | Client side graph labeling, including MCP related pages formatting |\n| Report which wikilinks in a document do not resolve to an existing document | `GET /api/docs/{doc_id}/broken-links` returning `BrokenLinkInfo[]` | Troubleshooting broken references during editing and after bulk imports |\n\nAccess control is applied consistently: document scoped reads first verify the caller can read the document, and global graph reads filter results so edges do not reveal documents outside the caller’s grants. Authentication and grant semantics are defined in [[Authentication & Authorization API]], and they influence both graph endpoints and helper endpoints such as `resolve` and `batch-titles`.\n\n## Freshness and rebuild expectations\n\nThe dependency graph is designed to stay current through normal write paths. When a document is created or updated, the backend extracts wikilinks from the new content, replaces that document’s outgoing edges, and then opportunistically refreshes incoming edges for forward references when a new target document appears.\n\nA full rebuild via `POST /api/dependencies/reindex` is primarily a repair tool for drift, bulk data loads, or manual database intervention, and it is intentionally restricted to admins. If graph views look stale in production, treat it like a regeneration signal and follow the operational checks in [[Jobs, Worker & Regeneration Pipeline]] alongside the symptom driven guidance in [[Maintenance, Backups & Runbooks]]; then confirm system health and database reachability through [[Monitoring & Health Checks]].\n\n## How MCP and the frontend use graph data\n\nThe MCP server exposes a related pages workflow that reads dependencies for a document, collects the referenced document IDs, and resolves human readable titles in one batch request before returning formatted markdown. That pattern avoids a per linked document lookup, while still keeping permission filtering on the server side through `batch-titles`, and it is implemented as part of [[MCP Server Integration]].\n\nThe frontend typically treats the dependency API as an edge provider. It can render a document centric view from `GET /api/docs/{doc_id}/dependencies`, and it can render a workspace wide graph by fetching `GET /api/dependencies` and then mapping IDs to titles through existing document listing or batch resolution endpoints, as described at a higher level in [[Frontend Architecture]].\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `backend/app/api/__init__.py` | Router exports showing dependencies and graph routers |\n| `backend/app/api/dependencies.py` | Dependency endpoints and global graph endpoints, including reindex |\n| `backend/app/api/documents.py` | Wikilink resolution, batch title lookup, and broken links endpoints |\n| `backend/app/schemas/dependency.py` | Response shapes for dependency and broken link payloads |\n| `backend/app/services/dependency_service.py` | Wikilink extraction, batch resolution, and dependency replacement logic |\n| `backend/app/services/document_service.py` | Create and update flows that refresh dependencies |\n| `backend/tests/test_dependencies_api.py` | Behavioral expectations for extraction, refresh, and endpoint responses |\n| `mcp-server/src/isocrates_mcp/server.py` | MCP tool flow that calls dependencies and batch title resolution |\n| `mcp-server/src/isocrates_mcp/formatters.py` | Formatting expectations for related and provenance outputs |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "api",
      "keywords": [
        "API",
        "Reference",
        "Endpoints"
      ],
      "description": "This page covers the Dependencies & Graph API in IsoCrates, including per document dependency retrieval, global graph retrieval for visualization, and administrative reindexing. It also explains how freshness is maintained on document writes and how the MCP server and frontend consume graph data.",
      "author_type": "seed"
    },
    {
      "title": "Database & Migrations",
      "path": "IsoCrates/backend",
      "content": "# Database & Migrations\n\nThis page explains how IsoCrates persists data in a relational database, how backups work for each supported database, and how schema changes are applied through migrations. It is written for operators and developers who need to reason about startup failures, safe upgrades, and restore verification in a system where documents, versions, and wikilink relationships are stored as rows rather than files.\n\n## Supported databases and backup implications\n\nIsoCrates supports SQLite and PostgreSQL via a `DATABASE_URL` connection string, which is the primary persistence configuration described in [[Configuration Overview]]. In practice, the operational difference is that SQLite backups are consistent file snapshots, while PostgreSQL backups are logical dumps that must be restored into a running database service.\n\n| Database | Example connection string (`DATABASE_URL`) | Backup method | Restore notes |\n|---|---|---|---|\n| SQLite | `sqlite:///./isocrates.db` | `./scripts/backup.sh` uses `sqlite3` with the `.backup` command to write a consistent `isocrates_<timestamp>.db` snapshot | Restore typically means replacing the database file while the backend is stopped, then starting the backend so [[Backend Service Lifecycle]] startup checks and migrations can run. The backup script resolves `./...` paths relative to `backend/`, so confirm the on-disk file location matches the URL you are using. |\n| PostgreSQL | `postgresql://user:password@host:5432/isocrates` | `./scripts/backup.sh` runs `pg_dump \"$DATABASE_URL\" | gzip` to produce `isocrates_<timestamp>.sql.gz` | Restore is environment-specific but usually involves creating an empty target database, piping the dump into `psql`, and then starting the backend so it can re-validate connectivity and apply any pending migrations. Treat restores as a change-management event and validate with health checks before promoting. |\n\n## Migration execution paths\n\nIsoCrates applies migrations in two distinct ways. The normal path is backend startup, where `backend/app/main.py` performs a database preflight query (`SELECT 1`) and then runs the built-in migrator before the FastAPI application serves traffic; this fail-fast behavior is intentional for production stability. The second path is a standalone SQLite-only script that applies a specific SQL file to the local database with a simple backup-and-rollback mechanism.\n\n| Path | When used | Failure mode | Logs to check |\n|---|---|---|---|\n| Backend boot migrations | Routine service start for both SQLite and PostgreSQL, as part of importing `backend/app/main.py` and running `run_migrations(engine, Base)` | Startup aborts with `SystemExit(1)` on either connectivity failure or migration failure, so the service crash-loops rather than partially serving | Backend logs around the startup probe message \"Connecting to database\" and the migration error message \"Database migration failed\" |\n| Standalone SQLite apply script | Manual, targeted SQLite schema change against `backend/isocrates.db`, typically during development or incident recovery | Script exits non-zero on SQL errors after restoring from `backend/isocrates.db.backup` | Terminal output from `apply_migration.py`, plus the presence of the `.backup` file for postmortem inspection |\n\nThe standalone script is intentionally minimal and expects you to provide a SQL file.\n\n```bash\ncd backend/migrations\npython apply_migration.py path/to/migration.sql\n```\n\n## Link to operational restore verification\n\nAfter any restore or manual migration, use the operator runbooks in [[Maintenance, Backups & Runbooks]] as the source of truth for end-to-end verification, because they cover not only database state but also derived artifacts and background processing that may need regeneration. In particular, the backend’s `GET /health` endpoint reports `db` status and a `document_count`, which makes it a practical guardrail for confirming that the restored database is reachable and that core tables contain plausible data; the expected semantics are detailed in [[Monitoring & Health Checks]].\n\nWhen troubleshooting a failed deploy that started after an upgrade, interpret backend startup crashes as either a `DATABASE_URL` connectivity problem or a migration problem, since both are executed before the app begins serving requests in the [[Backend Service Lifecycle]]. This division of responsibility keeps runtime behavior predictable, but it also means database issues should be addressed first, before investigating higher-level APIs such as [[Documents & Versions API]] or [[Wikilinks & Dependency Graph]].\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `backend/app/main.py` | Startup DB connectivity check, automatic migration invocation, and `/health` behavior |\n| `backend/migrations/apply_migration.py` | Standalone SQLite migration runner with backup and rollback |\n| `scripts/backup.sh` | SQLite and PostgreSQL backup creation and retention pruning |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "backend",
      "keywords": [],
      "description": "This page covers IsoCrates database persistence and how schema changes are applied, focusing on supported databases (SQLite and PostgreSQL), backup implications, and the two migration execution paths. It is aimed at operators and developers diagnosing startup failures and validating restores using the runbooks and health checks.",
      "author_type": "seed"
    },
    {
      "title": "Maintenance, Backups & Runbooks",
      "path": "IsoCrates/operations",
      "content": "# Maintenance, Backups & Runbooks\n\nThis page covers routine operations for IsoCrates, a documentation knowledge base that stores documents, versions, and wikilink relationships in a database and serves them through a FastAPI backend plus worker-driven regeneration. It is written for operators who need predictable backups, safe restores, and fast incident triage, while deeper design details live in [[Operations Overview]] and [[Backend Service Lifecycle]].\n\n## Backups (how, where, and how to verify)\n\nIsoCrates is primarily backed up by backing up the database because documents, versions, and dependency edges are persisted as rows rather than files. The repository includes `./scripts/backup.sh`, which reads `DATABASE_URL` from the environment or attempts to auto-detect it from `backend/.env`, then writes a timestamped artifact and prunes backups older than 30 days.\n\nRun the script from the repository root; the first example uses the default `./backups` directory and the second shows an explicit directory and `DATABASE_URL` override.\n\n```bash\n./scripts/backup.sh\n./scripts/backup.sh /var/backups/isocrates\nDATABASE_URL=postgresql://user:pass@db:5432/isocrates ./scripts/backup.sh /var/backups/isocrates\n```\n\nVerification is easiest when you treat every backup as disposable until it restores cleanly. Confirm the backup file size is non-trivial, then restore it into an isolated environment and verify `GET /health` reports `db=ok` and a plausible `document_count`; finally, confirm the worker can regenerate derived artifacts as described in [[Jobs, Worker & Regeneration Pipeline]].\n\n| Backup type | Artifact created by `backup.sh` | Verification steps (practical minimum) |\n|---|---|---|\n| SQLite (`DATABASE_URL=sqlite:///...`) | `isocrates_<timestamp>.db` created via `sqlite3 ... .backup` | Confirm the file exists and is not near-zero; restore into a disposable instance by pointing `DATABASE_URL` at the restored file; verify `GET /health` returns `status=healthy` and a plausible `document_count`. |\n| PostgreSQL (`DATABASE_URL=postgresql://...`) | `isocrates_<timestamp>.sql.gz` created via `pg_dump | gzip` | Confirm the gzip file is not near-zero and can be decompressed; restore into a new database and run the backend to apply migrations; verify `GET /health` returns `db=ok` and that reads in the frontend match expectations. |\n\n## Restore and post-restore validation\n\nRestores are environment-specific because they depend on how your database is provisioned and how secrets and connectivity are managed, as documented in [[Configuration Overview]] and [[Database & Migrations]]. Operationally, the backend is intentionally fail-fast: it validates database connectivity and runs migrations before serving traffic, so a restore that is missing credentials or points at an empty or incompatible database will typically show up immediately as a startup crash rather than a subtle partial outage.\n\nFor PostgreSQL backups created by `pg_dump`, the simplest restore shape is to create an empty target database, then load the dump and start the backend so migrations can run. For SQLite backups, restoring typically means replacing the database file and ensuring the path referenced by `DATABASE_URL` is readable and writable by the backend process. After any restore, prefer an explicit health check and interpret it using [[Monitoring & Health Checks]] before you declare success.\n\n```bash\ncurl -s http://localhost:8000/health\n```\n\nIn the JSON response, `status` should be `healthy` and `db` should be `ok`. If `status` is `degraded` or `db` is `error`, treat that as a database connectivity or schema problem and work through [[Database & Migrations]] first; `document_count` is a fast sanity check for “restored the wrong database” or “restored an empty snapshot.”\n\n## Symptom → checks (backend / worker / agent)\n\nMost incidents reduce to one of three failure domains: the backend cannot reach or migrate the database, the worker cannot claim or complete jobs, or the agent runtime cannot be invoked from the worker. Because the backend runs connectivity checks and migrations at import/startup time, backend failures often present as crash loops, while worker failures present as queues that never drain or jobs that fail with short captured error snippets.\n\n| Symptom | Checks to run first | Likely cause | Fix |\n|---|---|---|---|\n| Backend crash loop on startup | Confirm `DATABASE_URL` is set correctly and points to a reachable database; look for startup logs mentioning database connection verification or migration failure. | Database unreachable, wrong credentials, or migration error during startup validation. | Correct `DATABASE_URL` and database provisioning, then resolve migration issues per [[Database & Migrations]]; restart the backend after the database is reachable. |\n| `GET /health` returns `status=degraded` and `db=error` | Call `GET /health` repeatedly and compare with backend logs; verify the database is running and network-accessible from the backend container/host. | Database connection errors at request time, or permissions preventing `SELECT`/count queries. | Fix connectivity and credentials, then re-check health; if migrations are pending, restarting will apply them because migrations run during startup. |\n| Worker runs but no jobs complete | Check worker logs for “polling every 10s” and for repeated errors around claiming jobs; verify the worker can connect to the same database as the backend because it uses `SessionLocal`. | Worker misconfigured to point at a different database, or database connectivity issue specific to the worker environment. | Align the worker’s `DATABASE_URL` with the backend’s configuration and ensure database access; consult [[Jobs, Worker & Regeneration Pipeline]] for job lifecycle expectations. |\n| Jobs fail with “docker: not found” or “No such container: doc-agent” | Inspect worker environment variables `AGENT_MODE`, `AGENT_CONTAINER`, and `AGENT_SCRIPT_PATH`; confirm Docker is available to the worker when `AGENT_MODE=docker`. | Worker is configured for Docker exec mode but lacks Docker access or the agent container name differs. | Provide Docker access to the worker host/container and ensure the expected agent container is running, or switch to local mode with `AGENT_MODE=local` and a valid script path as described in [[Agent Pipeline Overview]]. |\n| Jobs fail quickly with a truncated Python traceback in the job error | Look at the last ~2000 characters of stderr captured for the job; confirm agent credentials and network access required for the repository and LLM provider. | Agent runtime misconfiguration, missing credentials, or repository access failures. | Fix agent configuration per [[Configuration Overview]] and regenerate; if failures persist, validate the agent invocation path described in [[Jobs, Worker & Regeneration Pipeline]]. |\n| Jobs time out after ~30 minutes | Confirm whether the repository is unusually large and whether the agent is making progress; review worker settings for timeout. | Worker kills the agent subprocess after `JOB_TIMEOUT_SECONDS` (default 1800s). | Increase `JOB_TIMEOUT_SECONDS` for large repos or reduce the job’s scope; validate the regeneration approach in [[Scouts & Planning]] to keep workloads bounded. |\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `scripts/backup.sh` | Backup command behavior, artifact naming, retention pruning |\n| `backend/app/main.py` | Startup database validation, migration behavior, and `/health` semantics |\n| `backend/worker.py` | Polling interval, daily refresh enqueueing, agent invocation and timeout behavior |\n| `backend/migrations/apply_migration.py` | SQLite migration application with backup-and-rollback safety |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "operations",
      "keywords": [],
      "description": "This page covers “Maintenance, Backups & Runbooks” for IsoCrates, focusing on database backups via `scripts/backup.sh`, how to validate restores using `GET /health`, and a compact symptom-to-checks triage table for backend, worker, and agent-runtime failures. It is aimed at operators maintaining availability and data integrity in production or staging deployments.",
      "author_type": "seed"
    },
    {
      "title": "Authentication & Authorization API",
      "path": "IsoCrates/api",
      "content": "# Authentication & Authorization API\n\nThis page covers how IsoCrates authenticates users and authorises access to documents and folders. It is relevant to operators enabling auth for the first time, developers integrating with the API, and administrators managing user permissions.\n\n## Authentication model\n\nIsoCrates uses hand-rolled JWT tokens with HMAC-SHA256 signatures. Tokens are issued on login and carry three claims: `sub` (user ID), `role` (global role), and `exp` (expiry, default 24 hours). The signing key is configured via `JWT_SECRET_KEY` and must be overridden from the default in production — the [[Backend Service Lifecycle]] blocks startup if the default key is used with `AUTH_ENABLED=true`.\n\nWhen `AUTH_ENABLED=false` (the default for development), all authentication dependencies return an anonymous admin context with a root grant, so every endpoint is fully accessible without a token. This is a deliberate convenience — see the [[Configuration Overview]] for switching to production mode.\n\n## Endpoints\n\n| Method | Path | Auth | Description |\n|---|---|---|---|\n| POST | `/api/auth/register` | Open (first user) / Admin | Create a new user account |\n| POST | `/api/auth/login` | None | Authenticate and receive a JWT token |\n| GET | `/api/auth/me` | Required | Get current user info and grants |\n| GET | `/api/auth/users` | Admin | List all user accounts |\n| PUT | `/api/auth/users/{id}/role` | Admin | Change a user's global role |\n| PUT | `/api/auth/users/{id}/deactivate` | Admin | Deactivate a user account |\n| POST | `/api/auth/users/{id}/grants` | Admin | Add or update a folder grant |\n| DELETE | `/api/auth/users/{id}/grants/{path}` | Admin | Revoke a folder grant |\n\nThe first registration is open to anyone and automatically creates an admin account. All subsequent registrations require admin authentication, preventing open self-registration in multi-user deployments.\n\n## Token lifecycle\n\nTokens are created by `token_factory.create_token()` and decoded by `token_factory.decode_token()`. Both functions are pure — they accept the secret key as an argument and hold no state. The decode function returns `None` on any failure (bad signature, expired, malformed) rather than raising exceptions, so callers simply treat absence as \"unauthenticated\".\n\nExample login flow:\n\n```bash\n# Authenticate and receive a token\ncurl -X POST http://localhost:8000/api/auth/login \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"email\": \"admin@example.com\", \"password\": \"your-password\"}'\n\n# Use the token on protected endpoints\ncurl http://localhost:8000/api/docs \\\n  -H 'Authorization: Bearer <token>'\n```\n\n## Authorization: path-based grants\n\nAfter authentication, authorization is handled by the [[Wikilinks & Dependency Graph]] of folder grants. Each grant is a pair of `(path_prefix, role)` that determines what a user can do under a specific folder path. The permission model follows three rules:\n\n1. **Longest prefix match wins** — if a user has a viewer grant on `\"\"` (root) and an editor grant on `\"backend/\"`, they can edit documents under `backend/` but only read everything else.\n2. **No matching grant = no access** — documents outside any granted prefix are invisible (the API returns 404, not 403, to prevent information leakage).\n3. **Role hierarchy** — admin > editor > viewer. Each role inherits the permissions of the roles below it.\n\n| Role | Allowed actions |\n|---|---|\n| admin | read, edit, delete, admin |\n| editor | read, edit, delete |\n| viewer | read |\n\nThe permission check is a single pure function in `permission_service.py` — the only place where permission rules are defined. The rest of the system calls `check_permission(grants, doc_path, action)` and acts on the boolean result.\n\n## FastAPI dependencies\n\nThe auth module exposes three FastAPI dependencies that endpoints inject:\n\n| Dependency | Behaviour | Use case |\n|---|---|---|\n| `require_auth` | Returns `AuthContext` or raises 401 | Write endpoints (POST, PUT, DELETE) |\n| `optional_auth` | Always returns `AuthContext`, never raises | Read endpoints (GET) — permission-filters results |\n| `require_admin` | Returns `AuthContext` or raises 403 | Admin-only endpoints (user management, grants) |\n\n`AuthContext` is a frozen dataclass containing `user_id`, `role`, and `grants`. It is created once per request and passed to service methods that need permission checks.\n\n## Key source files\n\n| File | Role |\n|---|---|\n| `backend/app/api/auth_routes.py` | Registration, login, user management, grant endpoints |\n| `backend/app/core/auth.py` | `require_auth`, `optional_auth`, `require_admin` dependencies |\n| `backend/app/core/token_factory.py` | Pure JWT create/decode functions (HMAC-SHA256) |\n| `backend/app/services/permission_service.py` | `check_permission()` — the single permission rule |\n| `backend/app/services/auth_service.py` | User registration, authentication, grant management |\n| `backend/app/models/user.py` | `User` and `FolderGrant` ORM models |",
      "repo_url": "https://github.com/nicobailon/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "api",
      "keywords": [
        "API",
        "Security"
      ],
      "description": "Authentication & Authorization API documents the IsoCrates auth system: JWT tokens, user registration and login endpoints, path-based folder grants, role hierarchy (admin/editor/viewer), and the dev-mode bypass that disables auth for local development.",
      "author_type": "seed"
    },
    {
      "title": "Operations Overview",
      "path": "IsoCrates/operations",
      "content": "# Operations Overview\n\nThis page summarizes the operational “steady state” for IsoCrates: which runtime components must be up, what partial outages look like, and which operator routines keep the system healthy. It is written for on-call operators and deployers who need a fast mental model, then want to drill into detailed procedures in [[Maintenance, Backups & Runbooks]] and subsystem details like [[Jobs, Worker & Regeneration Pipeline]].\n\n## What must be running (and what depends on what)\n\nIsoCrates serves a [Next.js](https://nextjs.org/) frontend through a [FastAPI](https://fastapi.tiangolo.com/) backend, with the database holding documents and versions described in [[Data Model Overview]]. Most outages reduce to whether the [[Backend Service Lifecycle]] can reach the database and complete fail-fast checks, including the connectivity probe and migrations described in [[Database & Migrations]]. The backend also runs best-effort housekeeping during startup, such as purging expired trash and optionally purging audit history, so misconfiguration often appears as a crash loop rather than a partially working API.\n\nThe regeneration worker is operationally independent from the request path, so the product can still read and edit documents while derived artifacts like the [[Wikilinks & Dependency Graph]] lag behind. However, the worker depends on both the database job queue described in [[Jobs, Worker & Regeneration Pipeline]] and an agent runtime invocation mode, so failures commonly show up as queued jobs that never start, repeated timeouts, or short stderr snippets captured in job records.\n\nThe diagram below shows the dependency edges you should consider when scoping an incident, including inbound integrations via [[Jobs & Webhooks API]] and operator tooling via the [[MCP Server Integration]].\n\n```mermaid\ngraph TD\n  U[User Browser] <--> F[Frontend (Next.js)]\n  F <--> B[Backend API (FastAPI)]\n  B <--> DB[(Database)]\n\n  W[Worker (job poller)] <--> DB\n  W --> AR[Agent runtime (local or Docker exec)]\n\n  INT[Integrations (webhooks, external automations)] --> B\n  MCP[MCP server] <--> B\n```\n\n## Routine operations checklist\n\nOperational work is mostly about verifying that the [[Configuration Overview]] still matches the environment, confirming backups, and watching the health and job signals that indicate the system is keeping derived data fresh. The backend’s health endpoint is intentionally resilient and returns a `degraded` status when it cannot reach the database, which makes it useful both for monitoring and for post-change validation.\n\nThe checklist table is a lightweight index into the deeper runbooks and hardening guidance, which are expanded in [[Maintenance, Backups & Runbooks]] and [[Deployment & Hardening Checklist]].\n\n| Cadence | Task | Success signal | Link |\n|---|---|---|---|\n| Each deploy or restart | Confirm backend startup completes database preflight and migrations | Backend stays up and logs show “Database connection verified” and no migration failure | [[Backend Service Lifecycle]] |\n| Daily | Verify runtime health for API and database reachability | `GET /health` reports `status=healthy`, `db=ok`, and a plausible `document_count` | [[Monitoring & Health Checks]] |\n| Daily | Ensure the regeneration loop is progressing | Job queue drains over time and new jobs transition from queued to completed | [[Jobs, Worker & Regeneration Pipeline]] |\n| Daily | Run a database backup using the repo script with the correct `DATABASE_URL` | A timestamped backup artifact is created and older backups are pruned per policy | [[Maintenance, Backups & Runbooks]] |\n| Weekly | Test a restore in an isolated environment, then validate health | Restored environment reports `db=ok` and expected document counts | [[Maintenance, Backups & Runbooks]] |\n| Monthly | Review hardening-sensitive configuration changes | Auth, secrets, and CORS settings match policy and do not rely on insecure defaults | [[Deployment & Hardening Checklist]] |\n\n## Incident navigation\n\nWhen diagnosing incidents, start by separating “request path” failures from “regeneration path” failures. A broken request path typically presents as frontend errors and a backend process that will not stay up, which is consistent with the backend’s strict startup behavior in the [[Backend Service Lifecycle]] and the fact that it validates database connectivity and migrations before serving traffic.\n\nNext, use health and logs to find the failing dependency edge. [[Monitoring & Health Checks]] centers on `GET /health`, which probes the database and returns `healthy` versus `degraded` along with a `document_count`, while [[Maintenance, Backups & Runbooks]] provides symptom-driven checks for common cases like bad `DATABASE_URL`, migration failures, or restore verification. If the core API is healthy but derived artifacts are stale or jobs are stuck, focus on the [[Jobs, Worker & Regeneration Pipeline]] and verify that the worker can access the database and successfully invoke the configured agent runtime mode.\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `notes/IsoCrates/operations/maintenance-backups--runbooks.md` | Operational runbooks, backup/restore guidance, symptom navigation |\n| `scripts/backup.sh` | Backup implementation, retention pruning, DATABASE_URL loading |\n| `backend/app/main.py` | Fail-fast DB preflight, migrations on startup, `/health` semantics |\n| `backend/worker.py` | Polling worker loop, daily refresh enqueueing, agent invocation modes |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "operations",
      "keywords": [],
      "description": "This Operations Overview page explains what must be running in an IsoCrates deployment, how the frontend, backend, database, worker, and agent runtime depend on each other, and what partial outages look like. It also summarizes routine operator checks and points incident response toward the health endpoint, backend startup behavior, and the detailed runbooks.",
      "author_type": "seed"
    },
    {
      "title": "Scouts & Planning",
      "path": "IsoCrates/agent",
      "content": "# Scouts & Planning\n\nThis page explains how the IsoCrates documentation agent turns a repository into a concrete documentation plan. It is written for maintainers working on the [[Agent Pipeline Overview]] and for operators debugging why the generated wiki has the pages, links, and scope that it does.\n\n## Scout outputs and strategy selection\n\nThe scout phase is Tier 0 of the agent pipeline: it explores a target repository and emits structured reports that the planner later summarizes into a blueprint. In `ScoutRunner.run()`, the runner first estimates repository size via `analyze_repository(...)`, computes `budget_ratio = token_estimate / scout_context_window`, and then chooses between topic-based and module-based scouting. This choice matters because smaller context budgets intentionally reduce the number of scouts executed, while larger budgets widen coverage and may switch to module-based exploration when enough modules are detected. When the combined scout output exceeds 20,000 characters, the runner produces compressed per-scout variants and exposes them via `compressed_text` and `compressed_reports_by_key`.\n\n| ScoutResult field | Meaning | Used by |\n|---|---|---|\n| `reports_by_key` | Primary scout reports keyed by scout name (for example, `structure` or `architecture`). | [[Agent Pipeline Overview]] writers and the [[Jobs, Worker & Regeneration Pipeline]] that persists scout artifacts. |\n| `compressed_reports_by_key` | Per-scout compressed variants used when the combined report text is large. | Writer briefs that need a shorter context window than the full reports. |\n| `combined_text` | All reports concatenated with separators for one-pass consumption. | [[Agent Pipeline Overview]] planning prompts and audit/debug logs. |\n| `compressed_text` | Concatenation of `compressed_reports_by_key` (or the original combined text if small). | Downstream writer prompts that must fit strict token budgets. |\n| `repo_metrics` | Repository sizing and manifest data including `token_estimate`, `file_count`, and `top_dirs`. | [[Architecture Principles & Conventions]] style decisions such as “split big topics into more pages.” |\n| `module_map` | Module summaries (names mapped to `ModuleInfo`) produced by repository analysis. | Module-based scouting decisions and module-scoped prompts. |\n| `budget_ratio` | `token_estimate / context_window`, which approximates how “tight” the scout budget is. | Strategy selection and scout coverage tuning. |\n\nThe following diagram captures the strategy thresholds implemented in `ScoutRunner.run()` and `_run_topic_scouts()`. It emphasizes that module-based scouting is only selected when the repository is both large relative to the context window and modular enough to justify splitting.\n\n```mermaid\ngraph TD\n  A[token_estimate from repo analysis] --> B[budget_ratio = token_estimate / scout_context_window]\n  B --> C{budget_ratio > 1.0\nand module_map has at least 4 modules?}\n  C -- yes --> D[module-based scouting]\n  C -- no --> E[topic-based scouting]\n  E --> F{budget_ratio < 0.3?}\n  F -- yes --> G[run only \"structure\" and \"architecture\"]\n  F -- no --> H{budget_ratio < 1.0?}\n  H -- yes --> I[run only scouts marked always_run]\n  H -- no --> J[run all scouts]\n```\n\n## Planner relevance and link hygiene\n\nThe planner phase is Tier 1: it consumes scout text and produces a JSON blueprint that names pages, paths, and intended cross-links. Because different page types need different scout inputs, `get_relevant_reports(doc_type, reports_by_key)` uses `SCOUT_RELEVANCE` to select the most useful report keys and then appends the `structure` report as a safety net even when it was not requested. This makes plans less brittle when a page is primarily “API” or “operations” focused but still needs repository orientation.\n\nWikilinks are treated as a correctness-sensitive contract rather than a stylistic flourish. The `sanitize_wikilinks(content, valid_titles, repo_url)` helper keeps `Title` and `Display` only when `Target` is present in `valid_titles`, otherwise it replaces the entire wikilink with plain display text; notably, `repo_url` is accepted but not currently used by the implementation. This behavior prevents planners and writers from emitting broken links that would pollute the wiki graph.\n\n```python\nfrom agent.planner import sanitize_wikilinks\n\nvalid_titles = {\"API Overview\", \"Documents & Versions API\"}\n\nbefore = \"See [[API Overview]] and Nonexistent Page and [[Documents & Versions API|Docs API]].\"\nafter = sanitize_wikilinks(before, valid_titles=valid_titles, repo_url=\"https://example.invalid/repo\")\n\n# after == \"See [[API Overview]] and Nonexistent Page and [[Documents & Versions API|Docs API]].\"\n```\n\n## How planned docs map to IsoCrates concepts\n\nA planner blueprint is not just a writing prompt; it is a schema for content that will be stored, versioned, and navigable inside IsoCrates. When the agent creates or updates pages through the [[Documents & Versions API]], each planned page becomes a first-class document with a version history that can be inspected and rolled back, which is why the planner prompt insists on stable titles and paths unless a rename is intentional.\n\nLink planning is equally structural. The blueprint’s intended wikilinks are later materialized as rows in the dependency graph, so link hygiene directly affects the system described in [[Wikilinks & Dependency Graph]] and the quality of navigation and related-document discovery. Renames and moves are handled by stable identity rather than filename heuristics, so planning needs to respect the metadata and update semantics described in [[Document Registry & Stable IDs]] to avoid creating duplicates when a page is reorganized.\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `agent/scout.py` | ScoutResult schema, budget-based strategy selection, compression threshold |\n| `agent/planner.py` | Scout relevance selection and `sanitize_wikilinks` implementation |\n| `agent/prompts.py` | Pipeline constants and planner taxonomy context |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "agent",
      "keywords": [],
      "description": "This page covers Scouts & Planning in the IsoCrates agent pipeline, explaining what ScoutRunner outputs, how budget_ratio selects a scouting strategy, and how the planner filters scout reports and sanitizes wikilinks. It is aimed at maintainers who need to reason about plan quality, link correctness, and how planned pages become stored wiki documents.",
      "author_type": "seed"
    },
    {
      "title": "Overview",
      "path": "IsoCrates",
      "content": "# Overview\n\n## What IsoCrates is (and isn’t)\n\nIsoCrates is a database backed documentation knowledge base that stores Markdown documents, their immutable versions, and wiki style links as first class data. It is designed for teams who want a navigable internal wiki with strong provenance and operational tooling, then want to automate parts of doc upkeep through an agent driven regeneration loop. For the product level scope and expected workflows, start with [[Capabilities & User Stories]] and then follow [[Getting Started]] to run it locally or in Docker.\n\nIsoCrates is not a file system notes vault or a static site generator that treats the Git repository as the source of truth; instead, the canonical state lives in the database, and the runtime serves content through an HTTP API and a web UI. It is also not “an LLM”: AI is an integration point that can propose or regenerate content, while humans can edit and the system preserves history through the versions feature. When you need day two guidance such as backups, restores, and symptom driven diagnosis, the operator entry point is [[Operations Overview]].\n\n## System diagram (components and data flow)\n\nIsoCrates runs as a small set of services with clear responsibility boundaries, typically deployed together with Docker Compose. The [FastAPI](https://fastapi.tiangolo.com/) backend fails fast at import time if the database cannot be reached or migrations cannot be applied, which makes startup errors visible early but also means misconfiguration tends to crash loop rather than degrade silently. The [Next.js](https://nextjs.org/) frontend focuses on document viewing and editing workflows such as version history, while background regeneration is handled by a separate worker that runs agent jobs sequentially.\n\nThe diagram below shows the primary data flows, including the optional MCP integration that lets external AI tools read and write documentation through a controlled tool surface.\n\n```mermaid\ngraph TD\n  U[User] -->|Browser| FE[Frontend (Next.js)]\n  FE -->|HTTP /api/*| BE[Backend API (FastAPI)]\n  MCP[MCP Server (stdio tools)] -->|HTTP API client| BE\n\n  BE -->|SQLAlchemy| DB[(PostgreSQL or SQLite)]\n\n  W[Worker (job poller)] -->|claim jobs, update status| DB\n  W -->|docker exec or subprocess| AR[Agent Runtime]\n  AR -->|create/update docs via HTTP| BE\n\n  BE -->|/health| MON[Monitoring / probes]\n```\n\nWhen you want a deeper understanding of how wikilinks become a navigable graph and how derived artifacts are regenerated, read [[Wikilinks & Dependency Graph]] alongside [[Jobs, Worker & Regeneration Pipeline]]. When you need the concrete REST surface area that the frontend, worker, and MCP server rely on, use [[API Overview]] as your map and then drill into [[Documents & Versions API]] and [[Dependencies & Graph API]].\n\n## Key components at a glance\n\nThe table below is meant to help you orient quickly in the repository, then choose the next page based on whether you are operating the system, extending the API, or working on the UI. Each “Primary pages” entry is a wiki jump point that explains the component’s behavior and contracts in more depth.\n\n| Component | Responsibility | Primary pages | Key files |\n|---|---|---|---|\n| Backend API service | Serves REST endpoints for documents, versions, and graphs; validates configuration and runs migrations at startup; exposes a resilient `/health` probe | [[Backend Service Lifecycle]], [[API Overview]], [[Database & Migrations]], [[Monitoring & Health Checks]] | `backend/app/main.py` |\n| Database | Stores documents, versions, link relationships, and job state as durable rows; acts as the operational source of truth for backups and restores | [[Data Model Overview]], [[Database & Migrations]], [[Maintenance, Backups & Runbooks]] | `backend/app/main.py` (connectivity probe), `scripts/backup.sh` |\n| Frontend web app | Provides document centric UX, including version history browsing, and calls the backend API as its system of record | [[Frontend Architecture]], [[Document Viewing, Editing & Versions UX]], [[Authentication & Authorization API]] | `frontend/app/docs/[docId]/versions/page.tsx` |\n| Worker | Polls for queued regeneration jobs, runs the agent in docker exec or local mode, and records truncated stderr on failures for debugging | [[Jobs, Worker & Regeneration Pipeline]], [[Operations Overview]] | `backend/worker.py` |\n| Agent pipeline and runtime | Generates or refreshes documentation for tracked repositories and pushes results back through the backend API with stable identifiers | [[Agent Pipeline Overview]], [[Scouts & Planning]], [[Document Registry & Stable IDs]] | `backend/worker.py` (agent invocation) |\n| MCP server | Exposes an MCP tool set for search, retrieval, and controlled writes so external AI editors can interact with IsoCrates via the backend API | [[MCP Server Integration]], [[API Overview]] | `mcp-server/src/isocrates_mcp/server.py` |\n| Backup tooling | Produces consistent database backups for SQLite and PostgreSQL and prunes old artifacts to support routine maintenance | [[Maintenance, Backups & Runbooks]], [[Operations Overview]] | `scripts/backup.sh` |\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `backend/app/main.py` | Backend startup validation, migrations, lifespan cleanup, `/health` semantics |\n| `backend/worker.py` | Job polling loop, daily refresh enqueueing, agent invocation modes and timeouts |\n| `scripts/backup.sh` | SQLite and PostgreSQL backup behavior and retention pruning |\n| `frontend/app/docs/[docId]/versions/page.tsx` | Frontend version history page behavior and data access pattern |\n| `mcp-server/src/isocrates_mcp/server.py` | MCP tool surface and title or ID resolution strategy |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "overview",
      "keywords": [
        "Overview",
        "Introduction"
      ],
      "description": "This Overview page introduces IsoCrates as a database-backed documentation knowledge base with a FastAPI backend, Next.js frontend, and a worker-driven regeneration loop that can invoke an agent runtime. It summarizes how the major services interact, points to the most important deeper wiki pages for APIs and operations, and orients readers to the key source files.",
      "author_type": "seed"
    },
    {
      "title": "Monitoring & Health Checks",
      "path": "IsoCrates/operations",
      "content": "# Monitoring & Health Checks\n\nThis page describes practical monitoring for IsoCrates operators, with an emphasis on the backend health contract and the signals that best predict user-visible failures. IsoCrates serves documents through a [FastAPI](https://fastapi.tiangolo.com/) backend, persists state in a relational database, and relies on background regeneration to keep derived artifacts current, so health checks should distinguish API availability from database reachability and from worker progress. For configuration and environment-specific wiring, treat this page as a companion to [[Operations Overview]] and [[Configuration Overview]].\n\n## Health endpoint semantics\n\nIsoCrates exposes `GET /health` for load balancers and for operator sanity checks after changes such as restores or migrations. The handler is intentionally defensive: it attempts a lightweight database probe and a document count, then returns a structured response even if the database call fails, which keeps upstream health probes stable while still surfacing degradation. When the backend cannot start at all due to database connectivity or migration failures, the process exits during startup, which is explained in [[Backend Service Lifecycle]] and operationalized in [[Database & Migrations]].\n\n| Field | Meaning | Alert guidance |\n|---|---|---|\n| `status` | Overall backend health classification, returned as `healthy` when the database probe succeeds and `degraded` when it fails. | Page on `degraded` if it persists beyond a short grace window, because core read and write paths depend on the database even if the HTTP process stays up. |\n| `db` | Database probe result, returned as `ok` when `SELECT 1` succeeds and `error` on any exception. | Treat `error` as a database reachability incident, and correlate with connection pool exhaustion and database restarts. |\n| `uptime_seconds` | Backend process uptime in seconds, measured from process start using a monotonic clock. | Alert on unexpected frequent resets, because they indicate crash loops or restarts that can interrupt writes and background cleanup. |\n| `version` | Backend API version string embedded in the running service. | Use for change correlation; unexpected version changes can explain sudden shifts in behavior or response shapes. |\n| `document_count` | Best-effort count of rows in the `documents` table, returned as zero when the database check fails. | Use as a coarse sanity check after restores; a sudden drop to near zero with `db=ok` is a data integrity concern and should trigger restore verification. |\n\n## Recommended monitors and alert thresholds\n\nMonitoring should pair simple probes with symptom-oriented signals. `GET /health` is a good first line because it distinguishes HTTP liveness from database reachability, but it should be complemented with error-rate and latency monitors that reflect real user experience. Because IsoCrates validates database connectivity and runs migrations before serving traffic, you should alert both on degraded health responses and on complete loss of the `/health` endpoint, since the latter often indicates a startup crash loop covered by [[Backend Service Lifecycle]].\n\n| Component | Signal | How to collect | Escalation path |\n|---|---|---|---|\n| Backend API | Health check state and body fields | Probe `GET /health` via your load balancer or synthetic monitor; track `status`, `db`, and `uptime_seconds`. | If `db=error`, follow database diagnostics in [[Maintenance, Backups & Runbooks]]; if the endpoint is unreachable, follow startup and migration triage in [[Backend Service Lifecycle]]. |\n| Backend API | Elevated 5xx rate or high latency | Use reverse proxy access logs and backend request logs; alert when sustained error rate or p95 latency exceeds your SLO. | Use request correlation and exception handling behavior described in [[Architecture Principles & Conventions]] to identify failing endpoints, then validate data-layer connectivity via `GET /health`. |\n| Database | Connection failures and restart events | Monitor PostgreSQL availability with standard database telemetry and connection checks aligned with `DATABASE_URL`. | If the backend cannot start, prioritize database reachability and credentials; migration failures should be handled via [[Database & Migrations]]. |\n| Job regeneration worker | Queue lag, timeouts, and failure messages | Monitor job age and failure rate via the jobs endpoint used for job status, then correlate with worker logs that show a 10 second polling loop and a 30 minute per-job timeout; inspect stored failure output, which is truncated to the last 2000 characters of stderr. | If jobs time out or error repeatedly while the API is healthy, treat it as a worker incident and follow the worker-focused procedures in [[Maintenance, Backups & Runbooks]] and the design constraints in [[Jobs, Worker & Regeneration Pipeline]]. |\n| Agent runtime integration | Agent invocation failures | In the default `AGENT_MODE=docker` configuration, verify that the worker can `docker exec` into the configured agent container and run the agent script; in `AGENT_MODE=local`, verify the worker host can run the agent subprocess directly. | If failures are environment-specific, validate the deployment posture in [[Deployment & Hardening Checklist]] and confirm the expected behaviors described in [[Agent Pipeline Overview]]. |\n| Backups | Backup freshness and restore rehearsal | Automate and monitor backup creation, then periodically restore into an isolated environment and validate with `GET /health` plus a plausible `document_count`. | If restore validation fails, treat it as an operational incident and follow [[Maintenance, Backups & Runbooks]] to prevent silent backup corruption. |\n\n## Triage shortcuts\n\nWhen `GET /health` returns `degraded`, start by treating it as a database reachability problem rather than an application bug, because the handler only flips state when the database probe fails. In contrast, when `/health` is unreachable, the backend likely never finished startup because database validation and migrations run before the service begins serving requests, which is why [[Backend Service Lifecycle]] is the most direct map from symptom to root cause.\n\nIf the UI feels stale while the API remains healthy, focus on the regeneration path that runs outside the request lifecycle. The worker polls for queued regeneration work every 10 seconds, processes jobs sequentially, and records a truncated stderr tail for failed runs, which makes job metadata and worker logs the fastest source of actionable clues; the operator-oriented model for that flow lives in [[Jobs, Worker & Regeneration Pipeline]] and the symptom-driven procedures live in [[Maintenance, Backups & Runbooks]].\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `backend/app/main.py` | Defines `GET /health` response fields and startup behavior |\n| `backend/worker.py` | Defines worker polling, timeouts, and error capture behavior |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "operations",
      "keywords": [],
      "description": "This page covers Monitoring & Health Checks for IsoCrates, focusing on the `GET /health` contract and how to interpret its fields for alerting. It also outlines a compact set of recommended monitors across the backend, database, and regeneration pipeline, with pointers to the most relevant operational runbooks.",
      "author_type": "seed"
    },
    {
      "title": "Backend Service Lifecycle",
      "path": "IsoCrates/backend",
      "content": "# Backend Service Lifecycle\n\nThis page describes how the IsoCrates backend starts, validates its environment, and manages its runtime lifecycle. It is written for operators deploying the service and developers modifying startup behaviour.\n\n## Startup sequence\n\nThe backend follows a fail-fast philosophy: if something essential is misconfigured the process exits immediately with a clear, actionable message rather than running in a broken state. The sequence is:\n\n1. **Logging initialisation** — `setup_logging()` configures structured JSON or text logging based on `LOG_LEVEL` and `LOG_FORMAT` before any other module runs.\n2. **Database validation** — `_validate_database_connection()` opens a connection and executes `SELECT 1`. On failure it logs a targeted diagnostic (wrong credentials, unreachable host, missing database) and calls `SystemExit(1)`. The connection URL is masked in logs so passwords never appear in output.\n3. **Migration execution** — `run_migrations()` detects whether this is a fresh install (no `documents` table) or an existing database. Fresh installs create all tables from SQLAlchemy models and baseline every migration; existing installs apply only pending migrations. Dialect-specific migrations (SQLite FTS5 vs PostgreSQL GIN/pgvector) are selected automatically. On failure a `MigrationError` halts startup.\n4. **Lifespan hook** — the `lifespan()` async context manager runs security validation, then yields to accept requests, and runs shutdown tasks when the process stops.\n\n## Security validation at startup\n\nInside `lifespan()`, the backend checks several security-sensitive settings:\n\n| Check | Development | Production |\n|---|---|---|\n| `JWT_SECRET_KEY` is the default | Warning logged | `ConfigurationError` raised, startup blocked |\n| `AUTH_ENABLED` is false | Warning logged | `ConfigurationError` raised |\n| `GITHUB_WEBHOOK_SECRET` is empty | Warning logged | Warning logged (webhooks still work, but signatures are not verified) |\n| `CORS_ALLOWED_ORIGINS` includes localhost | Warning logged | Blocked by Pydantic validator at import time |\n\nProduction validation is enforced by `settings.validate_production_config()`, which raises `ConfigurationError` for any combination that would leave the service insecure. Development mode logs warnings instead so the local workflow is unbroken.\n\n## Migration system\n\nThe migration runner in `core/migrator.py` supports both SQLite and PostgreSQL and handles three scenarios:\n\n**Fresh install** — all tables are created from SQLAlchemy models via `Base.metadata.create_all()`, and every discovered migration file is recorded in a `schema_migrations` tracking table without being executed. This avoids applying migrations whose changes are already captured in the current model definitions.\n\n**Existing database with tracking** — pending migrations are applied in version order. Each migration is a numbered SQL file (`NNN_description.sql`) in the `migrations/` directory. Dialect-specific migrations carry a `-- dialect: sqlite` or `-- dialect: postgresql` marker on the first line and are skipped on the wrong engine.\n\n**Existing database without tracking** — the runner inspects the live schema (column names, tables, indexes) to detect which migrations are already applied, baselines those, and then applies anything remaining. This upgrade path lets pre-tracking databases adopt the migration system without data loss.\n\nBefore applying migrations on SQLite the runner creates a timestamped backup (`isocrates.YYYYMMDD_HHMMSS.backup`). PostgreSQL deployments should use `pg_dump` externally.\n\n## Lifespan tasks\n\nAfter security validation, `lifespan()` performs two housekeeping tasks before yielding:\n\n- **Trash purge** — calls `DocumentService.purge_expired_trash()` to permanently delete documents whose `deleted_at` timestamp exceeds the retention period. Failures are logged but do not block startup.\n- **Audit log purge** — removes entries older than `AUDIT_RETENTION_DAYS` (default 365). Like trash purge, failures are non-fatal.\n\nShutdown currently has no cleanup tasks but the lifespan structure supports adding them.\n\n## Health endpoint\n\nThe `GET /health` endpoint runs a `SELECT 1` probe and a `SELECT COUNT(*) FROM documents` query, returning a JSON object with `status` (`\"healthy\"` or `\"degraded\"`), `db` (`\"ok\"` or `\"error\"`), `uptime_seconds`, `version`, and `document_count`. It never raises an exception — load balancers always receive a 200 response so they can distinguish an unhealthy service from one that is down.\n\n## Key source files\n\n| File | Role |\n|---|---|\n| `backend/app/main.py` | Application assembly, middleware stack, lifespan hook |\n| `backend/app/core/migrator.py` | Migration discovery, detection, and execution |\n| `backend/app/core/config.py` | Pydantic `Settings` class and production validation |\n| `backend/app/core/logging_config.py` | Structured logging setup with request-id context |\n| `backend/app/database.py` | Engine creation, `SessionLocal` factory, dialect detection |",
      "repo_url": "https://github.com/nicobailon/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "architecture",
      "keywords": [
        "Backend",
        "Architecture"
      ],
      "description": "Backend Service Lifecycle covers the startup sequence of the IsoCrates FastAPI backend, including database validation, migration execution, security checks, and the lifespan context manager that governs runtime behaviour and graceful shutdown.",
      "author_type": "seed"
    },
    {
      "title": "Data Model Overview",
      "path": "IsoCrates/architecture",
      "content": "# Data Model Overview\n\nIsoCrates treats documentation as first-class data: Markdown documents live in a path-based namespace, every edit is captured as a version, soft deletion preserves recoverability, and `wikilinks` are materialized into a queryable dependency graph. The primary REST representations for documents and their history are defined by the [[Documents & Versions API]]. This page summarizes the core entities stored by the Backend Architecture & FastAPI Lifecycle (a [FastAPI](https://fastapi.tiangolo.com/) service) and how they relate, so API consumers and UI/agent authors can reason about persistence without reading the full schema.\n\n## Entities and relationships\n\nThe data model centers on a `Document` identified by a stable string `id` (formatted like `doc-{hash}-{type}`) and located by a hierarchical `path` whose first segment acts as the top-level “crate”. A `Document` owns many immutable `Version` rows for history, and it participates in directed `Dependency` edges derived from `wikilinks`, which power the graph features described in [[Wikilinks & Dependency Graph]] and exposed through the [[Dependencies & Graph API]].\n\nFolder structure is modeled in two complementary ways: `path` prefixes group documents, while `FolderMetadata` stores optional attributes for folders (especially empty ones) that would not otherwise exist in a document-only tree, as surfaced by the Folders, Tree & Personal API. Users and access control live in `User` plus path-prefix `FolderGrant` rows, while `AuditLog` provides an immutable record of state-changing operations relevant to Webhook & API Security and operational review. Finally, `GenerationJob` represents webhook-triggered regeneration work that the system queues and executes via the job endpoints in [[Jobs & Webhooks API]] and the agent system described in Agent Pipeline Overview (Scout → Planner → Writer).\n\nThe diagram below is an ER-style map of the main tables and foreign keys.\n\n```mermaid\nerDiagram\n  DOCUMENT ||--o{ VERSION : has\n  DOCUMENT ||--o{ DEPENDENCY : from_doc\n  DOCUMENT ||--o{ DEPENDENCY : to_doc\n\n  USER ||--o{ FOLDER_GRANT : grants\n  USER ||--o{ AUDIT_LOG : records\n  USER ||--o{ PERSONAL_FOLDER : owns\n  PERSONAL_FOLDER ||--o{ PERSONAL_FOLDER : contains\n  USER ||--o{ PERSONAL_DOCUMENT_REF : curates\n  PERSONAL_FOLDER ||--o{ PERSONAL_DOCUMENT_REF : includes\n  DOCUMENT ||--o{ PERSONAL_DOCUMENT_REF : references\n\n  DOCUMENT {\n    string id PK\n    string path\n    string title\n    string doc_type\n    json keywords\n    text content\n    text content_preview\n    text description\n    string embedding_model\n    int generation_count\n    int version\n    datetime created_at\n    datetime updated_at\n    datetime deleted_at\n    text repo_url\n    string repo_name\n  }\n\n  VERSION {\n    string version_id PK\n    string doc_id FK\n    text content\n    string content_hash\n    string author_type\n    json author_metadata\n    datetime created_at\n  }\n\n  DEPENDENCY {\n    int id PK\n    string from_doc_id FK\n    string to_doc_id FK\n    string link_type\n    text link_text\n    string section\n    datetime created_at\n  }\n\n  FOLDER_METADATA {\n    string id PK\n    text path UK\n    text description\n    string icon\n    int sort_order\n    datetime created_at\n    datetime updated_at\n  }\n\n  USER {\n    string user_id PK\n    string display_name\n    string email UK\n    string role\n    bool is_active\n    datetime created_at\n  }\n\n  FOLDER_GRANT {\n    string user_id PK, FK\n    string path_prefix PK\n    string role\n    string granted_by FK\n    datetime created_at\n  }\n\n  AUDIT_LOG {\n    int id PK\n    string user_id FK\n    string action\n    string resource_type\n    string resource_id\n    text details\n    string ip_address\n    datetime created_at\n  }\n\n  PERSONAL_FOLDER {\n    string folder_id PK\n    string user_id FK\n    string name\n    string parent_id FK\n    int sort_order\n    datetime created_at\n  }\n\n  PERSONAL_DOCUMENT_REF {\n    string ref_id PK\n    string user_id FK\n    string folder_id FK\n    string document_id FK\n    int sort_order\n    datetime created_at\n  }\n\n  GENERATION_JOB {\n    string id PK\n    text repo_url\n    string commit_sha\n    string status\n    text error_message\n    int retry_count\n    datetime created_at\n    datetime started_at\n    datetime completed_at\n  }\n```\n\n## Lifecycle fields and invariants\n\nMost correctness in IsoCrates comes from a small set of lifecycle fields that multiple surfaces rely on, including the optimistic locking behavior described in Versioning & Conflict Resolution and the “existence hiding” behavior documented in API Overview & Conventions. In particular, `Document.deleted_at` implements soft deletion, `Document.version` supports write conflict detection, and the versioning tables use `ON DELETE CASCADE` to keep history and graph edges consistent when a document is permanently removed.\n\nThe table below summarizes the key invariants that keep the model predictable.\n\n| Invariant | Why it exists | Where enforced |\n|---|---|---|\n| A document is “active” only when `documents.deleted_at IS NULL`. | Enables trash/restore flows without losing IDs, history, and references. | Stored on `Document.deleted_at` and applied by indexing rules such as the FTS/embedding indexes in migrations. |\n| `documents.version` is an optimistic lock that increments on content updates; clients must present a matching version to update safely. | Prevents lost updates when multiple editors or agents save concurrently. | Defined as part of the `Document.version` contract, which notes that mismatches should yield HTTP 409 conflicts. |\n| Each `(from_doc_id, to_doc_id)` dependency edge is unique. | Keeps the `wikilinks` graph stable and avoids duplicate edges in graph visualizations. | Unique composite index `ix_dependencies_pair` in `Dependency` table args. |\n| Version history is append-oriented and belongs to a single document via `versions.doc_id`; deleting a document removes its versions. | Ensures auditability of edits while avoiding orphaned history on permanent delete. | Foreign key `versions.doc_id → documents.id` with `ondelete=\"CASCADE\"` in `Version`. |\n| Search indexes should ignore soft-deleted documents. | Avoids returning trashed pages in search, autocomplete, and related-content features. | SQLite FTS triggers only insert rows when `deleted_at IS NULL` and delete on update/delete (migration `010_add_fts5.sql`); Postgres HNSW index filters `deleted_at IS NULL` (migration `012_add_pgvector.sql`). |\n| Folder grants are path-prefix scoped, and the most specific prefix should win when multiple grants apply. | Makes permissions align with the document tree and supports delegation at subfolder granularity. | Captured in `FolderGrant.path_prefix` and described as a rule in the `FolderGrant` model docstring; permission behavior is surfaced through the Auth & Users API. |\n| Personal trees reference organizational documents rather than copying them. | Supports user-specific organization without fragmenting the canonical document content and version history. | `PersonalDocumentRef.document_id → documents.id` foreign key with `ondelete=\"CASCADE\"` in `PersonalDocumentRef`. |\n\n## Indexing and search (conceptual)\n\nIsoCrates treats “search” as a combination of classic text retrieval and semantic discovery, with the exact database mechanisms varying by engine; implementation details belong in Database & Migrations (Backend), while API and UX behavior are described in API Overview & Conventions and Navigation Surfaces (Tree, Search, Graph). On SQLite, migration `010_add_fts5.sql` creates a `documents_fts` virtual table backed by triggers that mirror inserts, updates, and deletions for active documents; this makes `title`, `content`, `path`, and flattened `keywords` searchable without scanning `documents`.\n\nOn PostgreSQL, migration `012_add_pgvector.sql` enables the `vector` extension and adds `documents.description_embedding vector(1536)` with an HNSW index filtered to active documents, supporting fast similarity queries over the AI-generated `Document.description`. At the model level, `Document.embedding_model` indicates which embedding model produced the stored vectors, and `Document.is_indexed` treats its presence as the “indexed for semantic search” flag; this aligns with agent and MCP behaviors that use descriptions for discovery, as covered in Agent Publishing: API Client & Doc Registry.\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `backend/app/models/document.py` | Document fields, lifecycle columns, and relationships |\n| `backend/app/models/version.py` | Version table schema and cascade semantics |\n| `backend/app/models/dependency.py` | Dependency edge schema and unique pair index |\n| `backend/app/models/folder_metadata.py` | Folder metadata schema for empty folders |\n| `backend/app/models/user.py` | User, folder grants, and audit log schemas |\n| `backend/app/models/personal.py` | Personal folders and document reference schema |\n| `backend/app/models/generation_job.py` | Regeneration job schema and status notes |\n| `backend/migrations/010_add_fts5.sql` | SQLite FTS5 virtual table and sync triggers |\n| `backend/migrations/012_add_pgvector.sql` | Postgres pgvector extension and HNSW index |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "data-model",
      "keywords": [
        "Data",
        "Schema",
        "Model"
      ],
      "description": "This page summarizes IsoCrates’ persisted data model, focusing on the main entities (documents, versions, dependencies, folders, users/grants, personal trees, audit logs, and generation jobs) and how they relate. It highlights key lifecycle invariants such as soft delete and optimistic locking and explains, at a conceptual level, how text and semantic indexing are supported across SQLite and PostgreSQL.",
      "author_type": "seed"
    },
    {
      "title": "Document Viewing, Editing & Versions UX",
      "path": "IsoCrates/frontend",
      "content": "# Document Viewing, Editing & Versions UX\n\nThis page describes the end-user experience for viewing documents, editing content, browsing version history, and using trash workflows in the IsoCrates Next.js frontend. It is written for engineers and designers who need to understand how the UI presents core backend concepts such as documents, immutable versions, and soft deletion; it also explains how the `docId` used in routes stays stable across edits, as defined in [[Document Registry & Stable IDs]]. The authoritative request and persistence behavior lives in [[Documents & Versions API]] and the underlying entities are summarized in [[Data Model Overview]].\n\n## Primary user flows (view, edit, version history, restore)\n\nDocument viewing and editing are designed around a stable document identity with an append-only history. When the user saves an edit, the backend updates the current document and also appends a new immutable version row, which is why the Version History screen can reliably show what changed over time.\n\nThe diagram below shows the read and edit loop as implemented by the backend routes under `/api/docs` and `/api/docs/{docId}/versions`.\n\n```mermaid\nsequenceDiagram\n  participant U as User\n  participant UI as Frontend (Next.js)\n  participant API as Backend API\n  participant DB as Database\n\n  U->>UI: Open /docs/{docId}\n  UI->>API: GET /api/docs/{docId}\n  API->>DB: Load Document (current content)\n  DB-->>API: Document\n  API-->>UI: Document\n  UI-->>U: Render current content\n\n  U->>UI: Edit and save\n  UI->>API: PUT /api/docs/{docId} (content, author metadata)\n  API->>DB: Update Document and create new Version\n  API->>DB: Refresh wikilink dependencies from new content\n  DB-->>API: Success\n  API-->>UI: Updated document\n\n  U->>UI: Verify in Version History\n  UI->>API: GET /api/docs/{docId}/versions\n  API->>DB: Load Versions (newest first)\n  DB-->>API: Versions\n  API-->>UI: Versions\n  UI-->>U: Show newest entry as “Current”\n```\n\nDeletion is modeled as a reversible soft delete exposed as `DELETE /api/docs/{docId}`, which sets `deleted_at` and makes the document appear in the Trash view. The Trash page supports restore via `POST /api/docs/{docId}/restore` and permanent deletion via `DELETE /api/docs/{docId}/permanent`; permanent deletion is admin-only in the backend, so non-admin users should expect the UI to show an error if they attempt it.\n\n```mermaid\nsequenceDiagram\n  participant U as User\n  participant UI as Frontend (Trash view)\n  participant API as Backend API\n  participant DB as Database\n\n  U->>UI: Delete document\n  UI->>API: DELETE /api/docs/{docId}\n  API->>DB: Soft-delete Document (set deleted_at)\n  DB-->>API: Success\n  API-->>UI: Success\n\n  U->>UI: Open /docs/trash\n  UI->>API: GET /api/docs/trash\n  API->>DB: Query deleted Documents\n  DB-->>API: Trashed documents\n  API-->>UI: Trashed documents\n\n  alt Restore\n    U->>UI: Restore\n    UI->>API: POST /api/docs/{docId}/restore\n    API->>DB: Restore Document (clear deleted_at)\n    DB-->>API: Success\n    API-->>UI: Updated document\n  else Permanent delete (admin)\n    U->>UI: Confirm permanent delete\n    UI->>API: DELETE /api/docs/{docId}/permanent\n    API->>DB: Delete Document and all Versions\n    DB-->>API: Success\n    API-->>UI: Success\n  end\n```\n\n## How the UI maps to backend concepts\n\nThe Version History list at `/docs/[docId]/versions` and the Historical Version viewer at `/docs/[docId]/versions/[versionId]` are server components that force dynamic rendering (`dynamic = 'force-dynamic'`). They map directly onto `GET /api/docs/{docId}/versions` and `GET /api/docs/{docId}/versions/{versionId}`, with the newest returned entry labeled “Current” and older snapshots clearly presented as historical.\n\nMarkdown rendering is handled by a client-side `MarkdownRenderer` that supports GitHub-flavored markdown, sanitization, Mermaid code blocks, and wiki-style links. It converts `Wiki Links` into clickable UI elements that call the backend resolver and then navigate to `/docs/{docId}`, which ties link behavior to the rules in [[Wikilinks & Dependency Graph]] and the endpoint semantics in [[Dependencies & Graph API]].\n\nThe Trash screen is also a client component: it fetches `/api/docs/trash`, manages bulk selection locally, and updates a global trash counter in the UI store. Retention and cleanup, including purging expired trash, is performed by the backend lifecycle described in [[Backend Service Lifecycle]] and operationally summarized in [[Maintenance, Backups & Runbooks]].\n\n| UI feature | API page | Data model concept |\n|---|---|---|\n| Current document view and edit/save | [[Documents & Versions API]] | Updating a Document also appends a new Version snapshot and refreshes dependency edges |\n| Version History list (`/docs/[docId]/versions`) | [[Documents & Versions API]] | Versions are immutable, ordered newest-first, and include `author_type` plus optional `author_metadata` |\n| Historical Version viewer (`/docs/[docId]/versions/[versionId]`) | [[Documents & Versions API]] | Historical content is retrieved by `version_id` and displayed with `content_hash` for traceability |\n| Trash listing (`/docs/trash`) | [[Documents & Versions API]] | Soft deletion sets `deleted_at` and the trash list queries deleted documents |\n| Restore from trash | [[Documents & Versions API]] | Restoring clears `deleted_at` while preserving the document id and version history |\n| Permanent delete from trash | [[Documents & Versions API]] | Permanent deletion removes the document and all versions, and the backend enforces admin-only access |\n| Wikilink navigation (`...` in markdown) | [[Dependencies & Graph API]] | A wikilink target is resolved to a `docId`, which represents an edge in the dependency graph |\n\n## Common UX problems and where to look\n\nMost UX failures in this area cluster into client-side state and selection behavior in React, server component data fetching, and backend persistence and authorization. Because reads are permission-scoped and often return 404 when access is denied, a “missing” document is sometimes an auth issue rather than a data issue.\n\nThe table below focuses on symptoms that commonly show up in the trash and version screens, and it points you to the most relevant subsystem page to continue the investigation.\n\n| Symptom | Likely layer | Next page |\n|---|---|---|\n| Version History does not include a recent save | Backend update and version creation path | [[Documents & Versions API]] |\n| Saving returns a 409-style conflict | Concurrent update protection (optimistic locking) | [[Documents & Versions API]] |\n| Permanent delete fails from the Trash screen | Admin-only endpoint or token/grant mismatch | [[Authentication & Authorization API]] |\n| Restore fails even though the item is visible in trash | Path-grant filtering or 404-on-deny behavior | [[Authentication & Authorization API]] |\n| Clicking a `wikilink` shows “not found” feedback | Resolver cannot map title to id, or caller cannot read target | [[Dependencies & Graph API]] |\n| Trash count is wrong after bulk restore/delete | Client state store update path | [[Frontend Architecture]] |\n| Permanently deleted documents appear again later | Regeneration job upsert recreates the stable id | [[Jobs, Worker & Regeneration Pipeline]] |\n\n## Sources\n\n| File | Purpose |\n|------|---------|\n| `frontend/app/docs/trash/page.tsx` | Trash UI behavior, selection, and bulk operations |\n| `frontend/app/docs/[docId]/versions/page.tsx` | Version history list rendering and “Current” labeling |\n| `frontend/app/docs/[docId]/versions/[versionId]/page.tsx` | Historical version viewer rendering and metadata display |\n| `frontend/components/markdown/MarkdownRenderer.tsx` | Markdown rendering, Mermaid blocks, and wikilink conversion |\n| `frontend/components/markdown/WikiLink.tsx` | Wikilink resolution call and client-side navigation |\n| `backend/app/api/documents.py` | Document update, trash listing, restore, and permanent delete endpoints |\n| `backend/app/api/versions.py` | Version listing and version retrieval endpoints |\n| `backend/app/services/document_service.py` | Version creation on update and trash lifecycle operations |\n",
      "repo_url": "https://github.com/Matthieu5555/IsoCrates",
      "repo_name": "IsoCrates",
      "doc_type": "frontend",
      "keywords": [],
      "description": "This page covers the Document Viewing, Editing & Versions UX in IsoCrates, including how saving edits appends new versions, how historical versions are rendered, how trash and restore work (including admin-only permanent deletion), and how `[[wikilinks]]` are resolved during reading. It is aimed at engineers and designers who are troubleshooting UX or API integration issues around viewing, editing, and version history.",
      "author_type": "seed"
    }
  ]
}