# IsoCrates Environment Configuration
# Copy this file to .env and fill in your values

# ============================================
# ENVIRONMENT (REQUIRED FOR PRODUCTION)
# ============================================
# Set to "production" for production deployments.
# In production, the app will FAIL TO START if:
#   - JWT_SECRET_KEY uses the default insecure value
#   - AUTH_ENABLED is false
#   - CORS allows localhost origins
ENVIRONMENT=development

# ============================================
# DATABASE CONFIGURATION
# ============================================
# SQLite (local development):
DATABASE_URL=sqlite:///./isocrates.db
# PostgreSQL (docker-compose / production):
# DATABASE_URL=postgresql://isocrates:isocrates_dev@localhost:5432/isocrates

# ============================================
# CORS CONFIGURATION (SECURITY)
# ============================================
# Comma-separated list of allowed origins
# DO NOT use wildcard (*) in production
CORS_ALLOWED_ORIGINS=http://localhost:3000,http://localhost:3001

# ============================================
# LOGGING
# ============================================
LOG_LEVEL=INFO

# ============================================
# AGENT CONFIGURATION
# ============================================
DOC_API_URL=http://backend-api:8000

# ============================================
# LLM CONFIGURATION
# ============================================
# Global defaults — each tier (scout/planner/writer) falls back to these.
# Override per-tier with SCOUT_MODEL, SCOUT_BASE_URL, SCOUT_API_KEY, etc.

# All four are REQUIRED — no defaults. Pick your provider:
#
# --- Ollama (local, free) ---
# LLM_BASE_URL=http://localhost:11434
# SCOUT_MODEL=ollama_chat/qwen3-coder:30b
# PLANNER_MODEL=ollama_chat/mistral-small:24b
# WRITER_MODEL=ollama_chat/qwen3-coder:30b
#
# --- OpenRouter (cloud) ---
# LLM_BASE_URL=https://openrouter.ai/api/v1
# LLM_API_KEY=sk-or-v1-your-api-key-here
# SCOUT_MODEL=openrouter/mistralai/devstral-2512
# PLANNER_MODEL=openrouter/moonshotai/kimi-k2-thinking
# WRITER_MODEL=openrouter/mistralai/devstral-2512

# --- Mix providers (e.g. local scouts, cloud planner) ---
# LLM_BASE_URL=http://localhost:11434
# SCOUT_MODEL=ollama_chat/qwen3-coder:30b
# PLANNER_MODEL=openrouter/anthropic/claude-3.5-sonnet
# PLANNER_BASE_URL=https://openrouter.ai/api/v1
# PLANNER_API_KEY=sk-or-v1-your-api-key-here
# WRITER_MODEL=ollama_chat/qwen3-coder:30b

# Disable native tool calling for models that don't support it
# LLM_NATIVE_TOOL_CALLING=false

# Number of parallel writer agents (default: 3, set to 1 for sequential)
# WRITER_PARALLEL=3

# Docker secrets (production)
# OPENROUTER_API_KEY_FILE=/run/secrets/openrouter_api_key

# Legacy alias (still works as fallback for LLM_API_KEY)
# OPENROUTER_API_KEY=sk-or-v1-your-api-key-here

SANDBOX_TYPE=local

# ============================================
# EMBEDDING CONFIGURATION (Semantic Search)
# ============================================
# Provider-agnostic via LiteLLM. Supports OpenAI, Cohere, Ollama, and more.
# Leave EMBEDDING_MODEL empty to disable embeddings (FTS-only search).
# Requires PostgreSQL with pgvector extension for vector storage.
#
# --- OpenAI ---
# EMBEDDING_MODEL=openai/text-embedding-3-small
# EMBEDDING_API_KEY=sk-your-openai-key
#
# --- Cohere ---
# EMBEDDING_MODEL=cohere/embed-english-v3.0
# EMBEDDING_API_KEY=your-cohere-key
#
# --- Ollama (local, free) ---
# EMBEDDING_MODEL=ollama/nomic-embed-text
# EMBEDDING_API_BASE=http://localhost:11434
#
# --- Custom OpenAI-compatible endpoint ---
# EMBEDDING_MODEL=openai/my-model
# EMBEDDING_API_BASE=https://my-provider.com/v1
# EMBEDDING_API_KEY=my-key
#
# Dimensions (0 = use model default, e.g. 1536 for text-embedding-3-small)
# EMBEDDING_DIMENSIONS=0
